#!/usr/bin/python
###############################################################################
#
# amplishot.py - Pipeline for generating full-length 16S sequences and performing
#                community abundance measurements on the output#
###############################################################################
#                                                                             #
#    This program is free software: you can redistribute it and/or modify     #
#    it under the terms of the GNU General Public License as published by     #
#    the Free Software Foundation, either version 3 of the License, or        #
#    (at your option) any later version.                                      #
#                                                                             #
#    This program is distributed in the hope that it will be useful,          #
#    but WITHOUT ANY WARRANTY; without even the implied warranty of           #
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the            #
#    GNU General Public License for more details.                             #
#                                                                             #
#    You should have received a copy of the GNU General Public License        #
#    along with this program. If not, see <http://www.gnu.org/licenses/>.     #
#                                                                             #
###############################################################################

__author__ = "Connor Skennerton"
__copyright__ = "Copyright 2012-2013"
__credits__ = ["Connor Skennerton"]
__license__ = "GPL3"
__version__ = "0.0.1"
__maintainer__ = "Connor Skennerton"
__email__ = "c.skennerton@gmail.com"
__status__ = "Development"

###############################################################################

import argparse
import sys
import tempfile
import os
import shutil
import multiprocessing
#import threading
import subprocess

import amplishot.parse.fastx 
from amplishot import phrap, pandaseq, conserved_sequences, taxon_segregator
import amplishot.search.wumanber as wumanber

###############################################################################
###############################################################################
###############################################################################
###############################################################################

###############################################################################
###############################################################################
###############################################################################
###############################################################################
def panda(args):
    panda = pandaseq.Pandaseq()
    panda.Parameters['-o'].on(args.olap)
    panda.Parameters['-l'].on(args.minlen)
    panda.Parameters['-f'].on(args.read_1)  
    panda.Parameters['-r'].on(args.read_2)
    return panda()

#def make_partition_files(names, directory=None):
#    ret = dict()
#    outdirs = list()
#    for i in names:
#        try:
#            shutil.rmtree(os.path.join(directory,i))
#        except:
#            pass
#        finally:
#            os.mkdir(os.path.join(directory,i))
#        outdirs.append(os.path.join(directory,i))
#        ret[i] = open(os.path.join(directory,i,'in.fa'), 'w')
#        ret[i+'qual'] = open(os.path.join(directory,i,'in.fa.qual'), 'w')
#
#    try:
#        shutil.rmtree(os.path.join(directory,'nomatch'))
#    except:
#        pass
#    finally:
#        os.mkdir(os.path.join(directory,'nomatch'))
#        
#    outdirs.append(os.path.join(directory,'nomatch'))
#    ret['nomatch'] = open(os.path.join(directory, 'nomatch','in.fa'), 'w')
#    ret['nomatchqual'] = open(os.path.join(directory, 'nomatch','in.fa.qual'), 'w')
#        
#    return ret, outdirs

def map_to_reference(inreads, bowtiedb, threads):
    """Take overlapped reads from pandaseq and map them to reference sequences
       returns path to output file containing sam formated alignments
       inreads: path to file containing overlapped reads from pandaseq
       bowtiedb: path to fileprefix of the bowtie database
    """
    sam = tempfile.NamedTemporaryFile(suffix='.sam', delete=False)
    with open(os.devnull, 'w') as fnull:
        subprocess.call(['bowtie2', '-U',inreads, '-x', bowtiedb, '-p',
            str(threads)], stdout = sam,
            stderr = fnull)
    
    sam.close()
    return sam.name

def taxon_partition(samfile, taxonomyFile, rootdir='root'):
    """Call the taxon segregator and return a list of tuples of taxonomies that
       have adequate coverage for assembly
    """
    taxseg = amplishot.taxon_segregator.TaxonSegregator(taxonomyFile)
    taxseg.parse_sam(samfile)
    return taxseg.segregate(root=rootdir)

#def partition(infp, tempdir=None):
#    """split the dataset based on the presence of conserved marker genes
#       infile - input file object
#       partitionFiles - a dictionary containing the file names for outputing
#       partitioned fasta reads
#       tempdir - output directory for partitions, if not set makes a tempdir
#    """
#    if tempdir is None:
#        tempdir = tempfile.mkdtemp()
#
#    tcs = conserved_sequences.ConservedSequences()
#    outfiles, outdirs = make_partition_files(tcs.positions, tempdir)
#
#    seqs = tcs.conserved_sequences.keys()
#    wu = wumanber.WuManber(seqs)
#    count = 0
#    total_count = 0
#    if isinstance(infp, str):
#        infp = open(infp)
#    
#    for name, seq, qual in MinimalFastqParser(infp, strict=False):
#        total_count += 1
#        qual = map(lambda x: ord(x)-33, qual)
#        qual_str = ' '.join(map(str,qual))
#        ret = wu.search_text(str(seq))
#        if ret is not None:
#            count += 1
#            # write the fasta sequence file
#            outfiles[str(tcs.conserved_sequences[ret[1]].pos)].write('>%s\n%s\n' % (name, seq))
#            # write the fasta qual file
#            outfiles[str(tcs.conserved_sequences[ret[1]].pos)+'qual'].write('>%s\n%s\n'
#                    % (name, qual_str))
#        else:
#            outfiles['nomatch'].write('>%s\n%s\n' % (name, seq))
#            outfiles['nomatchqual'].write('>%s\n%s\n' % (name, qual_str))
#
#    for f in outfiles.values():
#        f.close()
#
#    if count == total_count:
#        shutil.rmtree(os.path.join(tempdir,'nomatch'))
#        outdirs.remove(os.path.join(tempdir,'nomatch'))
#
#    print 'Able to Partition %i (%.2f %%) reads into %i bins' % (count,
#            (count / total_count)*100, len(outdirs) - 1 )
#    return outdirs

def cd_hit_reduce(workingdir, infile='in.fa', outfile='cdhitout.fa'):
    #print ' '.join(["cd-hit-est","-c","0.98", "-M", "1000","-o", os.path.join(workingdir,outfile),"-i",os.path.join(workingdir,infile)])
    with open(os.devnull, "w") as fnull:
        return subprocess.call(["cd-hit-est","-c","0.98", "-M", "1000","-o",
                os.path.join(workingdir,outfile),"-i",os.path.join(workingdir,infile)],
            stdout=fnull, stderr=fnull)

#def extract_subset(headers, dbfp, parser=MinimalFastaParser, outfp=None, nameTransform=None):
#    """nameTransform is a function to modify the names of the output
#    """
#    return_dict = dict()
#    for name, seq in parser(dbfp):
#        if name in headers:
#            if nameTransform is None:
#                if outfp is None:
#                    return_dict[name] = seq
#                else:
#                    outfp.write('>%s\n%s\n' % (name, seq))
#            else:
#                if outfp is None:
#                    return_dict[nameTransform(name)] = seq
#                else:
#                    outfp.write('>%s\n%s\n' % (nameTransform(name), seq))
#    return return_dict
#
#
def process_cd_hit_results(directory, cdhitout = 'cdhitout.fa', cdhitin = 'in.fa'):
    clustered_qual = open(os.path.join(directory,cdhitout+'.qual'), 'w')
    clustered_reads = set()
    cd_hit_fasta = open(os.path.join(directory,cdhitout))
    fxparser = amplishot.parse.fastx.FastxReader(cd_hit_fasta)
    for name, seq, qual in fxparser.parse():
        clustered_reads.add(name)

    cd_hit_fasta.close()
    inqual = open(os.path.join(directory,cdhitin+'.qual'))
    qualparser = amplishot.parse.fastx.QualityReader(inqual)
    for name, qual in qualparser.parse():
        if name in clustered_reads:
            clustered_qual.write('>%s\n%s\n' % (name, qual))
    inqual.close()


def phrap_assemble(workingdir, infile='cdhitout.fa', suppressStdout=True, suppressStderr=True):
    out = None
    err = None
    if suppressStdout:
        out = open(os.devnull, "w")
    if suppressStderr:
        err = open(os.devnull, "w")
    print ' '.join(['phrap',infile,'-minscore','300'])
    return subprocess.call(['phrap',infile,'-minscore','300'],
            stderr=err, stdout=out, cwd=workingdir)

#def combine_partitions(workingdir, outreads, outqual,
#        prefix='cdhitout.fa', outprefix='read'):
#    counter = 0
#    qualcounter = 0
#    # read in the contigs and rename them as we go
#    readsfp = open(os.path.join(workingdir,prefix+'.contigs'))
#    qualfp = open(os.path.join(workingdir,prefix+'.contigs.qual'))
#    singletsfp = open(os.path.join(workingdir, prefix+'.singlets'))
#    qualdbfp = open(os.path.join(workingdir, prefix+'.qual'))
#    
#    for name, seq in MinimalFastaParser(readsfp):
#        counter += 1
#        outreads.write('>%s\n%s\n' % (outprefix+str(counter), seq))
#    
#    for name, qual in MinimalFastaQualityParser(qualfp):
#        qualcounter+=1
#        outqual.write('>%s\n%s\n' % (outprefix+str(qualcounter), qual))
#    
#    if counter != qualcounter:
#        raise RuntimeError, "counter mismatch"
#        
#    singlets = dict()
#    for name, seq in MinimalFastaParser(singletsfp):
#        singlets[name] = [seq,'']
#    
#    for name, qual in MinimalFastaQualityParser(qualdbfp):
#        try:
#            singlets[name][1] = qual
#        except:
#            pass
#            
#    for name, data in singlets.items():
#        counter+=1
#        outqual.write('>%s\n%s\n' % (outprefix+str(counter), data[1]))
#        outreads.write('>%s\n%s\n' % (outprefix+str(counter), data[0]))
#
#    readsfp.close()
#    qualfp.close()
#    singletsfp.close()
#    qualfp.close()

###############################################################################
###############################################################################
###############################################################################
###############################################################################

if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument('-T','--taxonomy', dest='taxonomy', help="File\
            containing the taxonomy mapping of the reference sequences in the\
                    bowtie database")
    parser.add_argument('-d', '--bowtie-database', dest='bowtiedb', help="the\
            path to the bowtie database, without the suffixes; just like you\
            would write when using bowtie")
    parser.add_argument('-i', '--infile', dest='infile', 
            help="input file containing overlapped reads from pandaseq")
    parser.add_argument('-t', '--threads', dest='threads', type=int, default=1,
            help="The number of threads to use for processing")
    
    # parse the arguments
    args = parser.parse_args()        
    
    root_dir = './'
    taxon_root = os.path.join(root_dir, 'root')
    # generate fragment consensus sequences - pandaseq
    #panda_out = panda(args)

    # map overlapped fragments with bowtie
    samfile = map_to_reference(args.infile, args.bowtiedb, args.threads)

    # partition dataset based on mapped reads
    good_taxons, bad_taxons = taxon_partition(samfile, args.taxonomy,
            rootdir=taxon_root)
    print 'There are %i taxons suitable for assembly and %i taxons with\
            incomplete coverage' % (len(good_taxons), len(bad_taxons))
    if len(good_taxons) < 1:
        print "No taxons for assembly"
        sys.exit(1)

    good_taxons = [os.path.join(taxon_root, *x) for x in good_taxons]
    # dataset partitioning based on conserved primer presence
    #partitions = partition(args.infile, root_dir)
    
    # dataset reduction - cd-hit - each partition - threaded
    #pool = multiprocessing.Pool(processes=args.threads)
    for t in good_taxons:
     #   print "parallel cd-hit clustering..."
      #  pool.apply_async(cd_hit_reduce, t, {'infile':'reads.fa'})
      cd_hit_reduce(t, infile='reads.fa')
    #pool.close()
    #pool.join()

    for t in good_taxons:
        process_cd_hit_results(t, cdhitin='reads.fa')

    ## generate overlaps - phrap - each partition - threaded
    for t in good_taxons:
        phrap_assemble(t)
    #pool = multiprocessing.Pool(processes=args.threads)
    #pool.map_async(phrap_assemble, good_taxons)
    #pool.close()
    #pool.join()

    #combined assemblies - phrap
   # combined_dir = os.path.join(root_dir, 'combined')
   # try:
   #     shutil.rmtree(combined_dir)
   # except:
   #     pass
   # finally:
   #     os.mkdir(combined_dir)
   # combined_out_file = os.path.join(combined_dir,'combined.fa')
   # combined_reads = open(combined_out_file, 'w')
   # combined_qual = open(os.path.join(combined_out_file+'.qual'),'w')
   # 
   # for part in partitions:
   #     combine_partitions(part, combined_reads, combined_qual,
   #             outprefix=os.path.basename(part) )

   # combined_reads.close()
   # combined_qual.close()
   # 
   # phrap_assemble(combined_dir, infile='combined.fa')
    #chimera checking - uchime, decipher, chimeraslayer

    #read alignment - bwa

    #generate abundance based on coverage

    #assign taxonomy

    #generte OTU table


###############################################################################
###############################################################################
###############################################################################
###############################################################################
