#!/usr/bin/python
###############################################################################
#
# amplishot.py - Pipeline for generating full-length 16S sequences and
#                performing community abundance measurements on the output
#
###############################################################################
#                                                                             #
#    This program is free software: you can redistribute it and/or modify     #
#    it under the terms of the GNU General Public License as published by     #
#    the Free Software Foundation, either version 3 of the License, or        #
#    (at your option) any later version.                                      #
#                                                                             #
#    This program is distributed in the hope that it will be useful,          #
#    but WITHOUT ANY WARRANTY; without even the implied warranty of           #
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the            #
#    GNU General Public License for more details.                             #
#                                                                             #
#    You should have received a copy of the GNU General Public License        #
#    along with this program. If not, see <http://www.gnu.org/licenses/>.     #
#                                                                             #
###############################################################################

__author__ = "Connor Skennerton"
__copyright__ = "Copyright 2012-2013"
__credits__ = ["Connor Skennerton"]
__license__ = "GPL3"
__version__ = "0.4.0"
__maintainer__ = "Connor Skennerton"
__email__ = "c.skennerton@gmail.com"
__status__ = "Development"

###############################################################################
import logging
import argparse
import sys
import tempfile
import os
import time
import shutil
import subprocess
import textwrap
import gzip
import multiprocessing
from collections import defaultdict
import amplishot.parse.fastx
from amplishot import taxon_segregator, otu_table, assign_taxonomy, assemble,\
        OrientationChecker
from amplishot.app import bowtie, pandaseq, sortmerna
from amplishot.config import AmplishotConfig
from amplishot.app.cd_hit import CD_HIT_EST
from amplishot.util import reverse_complement
from cogent.app.util import FilePath
###############################################################################


def map_to_reference(params=None, stderr=None,
        stdout=None):
    """Take overlapped reads from pandaseq and map them to reference sequences
       returns path to output file containing sam formated alignments
       inreads: path to file containing overlapped reads from pandaseq
       bowtiedb: path to fileprefix of the bowtie database
    """
    b = amplishot.app.bowtie.Bowtie2(params=params, SuppressStderr=True,
            SuppressStdout=True)
    logging.debug(str(b))
    return b(stderr=stderr, stdout=stdout)


def taxon_partition(samfile, taxonomyFile, rootdir='root'):
    """Call the taxon segregator and return a list of tuples of taxonomies that
       have adequate coverage for assembly
    """
    taxseg = amplishot.taxon_segregator.TaxonSegregator(taxonomyFile)
    taxseg.parse_sam(samfile)
    return taxseg.segregate(root=rootdir)


def demultiplex(args):
    pass


def overlap(forward, reverse, pairtig_dir, minPairtigLen=350, minOverlap=30,
        threads=1):
    pear_basename =  os.path.join(pairtig_dir,
           os.path.splitext(os.path.basename(forward))[0])
    # pear doesn't like gziped files therefore unzip them at teh start
    gzip_cmd = 'gzip -dc %s > %s'
    if forward[-2:] == 'gz':
        tmp_forward = os.path.join(pairtig_dir, '__ft.fastq')

        retcode = subprocess.call(gzip_cmd % (forward, tmp_forward), shell=True)
        if retcode != 0:
            raise RuntimeError("unzipping the forwad reads was unsuccessful")
        forward = tmp_forward

    if reverse[-2:] == 'gz':
        tmp_reverse = os.path.join(pairtig_dir, '__rt.fastq')
        retcode = subprocess.call(gzip_cmd % (reverse, tmp_reverse), shell=True)
        if retcode != 0:
            raise RuntimeError("unzipping the reverse reads was unsuccessful")
        reverse = tmp_reverse

    pear_cmd = "pear -f %s -r %s -o %s -n %d -v %d -j %d" % (forward, reverse,
           pear_basename, minPairtigLen, minOverlap, threads )
    with open(os.devnull, 'wb') as devnull:
        retcode = subprocess.call(pear_cmd, shell=True, stdout=devnull, stderr=devnull)
        if retcode != 0:
            raise RuntimeError("pear did not exit correctly: %d.\n%s\n" %(retcode, pear_cmd))

    # clean up the temp files
    if forward == os.path.join(pairtig_dir, '__ft.fastq'):
        os.remove(forward)
    if reverse == os.path.join(pairtig_dir, '__rt.fastq'):
        os.remove(reverse)

    return pear_basename + '.assembled.fastq'


def filter_sam(args):
    taxseg = amplishot.taxon_segregator.TaxonSegregator(args.taxonomy)
    for samfile in args.sam:
        taxseg.parse_sam(samfile, args.percentid)
    good_taxons, bad_taxons = taxseg.segregate(root=args.outdir,
            minCount=args.taxon_width,
            minCoverage=args.taxon_depth)

def make_aliases(config):
    indata = []
    config.data['aliases'] = []
    if 'skip_pairtigs' not in config.data:
        for readfile in config.data['pairtig_read_files']:
            sample_file_name =\
            os.path.splitext(os.path.basename(readfile))[0]
            indata.append([readfile, sample_file_name])
            config.data['aliases'].append(sample_file_name)
    else:
        for r1, r2 in config.data['input_raw_reads']:
            sample_file_name =\
            os.path.splitext(os.path.basename(r1))[0]
            indata.append(([r1,r2], sample_file_name))
            config.data['aliases'].append(sample_file_name)

    return indata


def orientate(fullLengthSeqs):
    cs = OrientationChecker.OrientationChecker()
    for name in fullLengthSeqs.keys():
        if cs.isSeqReversed(fullLengthSeqs[name]):
            fullLengthSeqs[name] = reverse_complement(fullLengthSeqs[name])
    
    return fullLengthSeqs


def count_reads_in_file(fp):
    parser = amplishot.parse.fastx.FastxReader(fp)
    read_count = 0
    for name, seq, qual in parser.parse():
        read_count += 1
    return read_count


def generate_pairtigs(config, root_dir):
    reads_counts = []
    if len(config.data['pairtig_read_files']) > 0:
        for pairtig_file in config.data['pairtig_read_files']:
            with open(pairtig_file) as fp:
                reads_counts.append(count_reads_in_pairtig_file(fp))
    else:
        pairtig_dir = os.path.join(root_dir, 'pairtigs')
        if not os.path.exists(pairtig_dir):
            os.mkdir(pairtig_dir)

        logging.info('generating pairtigs...')
        # lets not fork unnessessary processes
        panda_processes = len(config.data['input_raw_reads']) if\
        len(config.data['input_raw_reads']) < config.data['threads'] else\
        config.data['threads']

        panda_pool_results = []
        for f, r in config.data['input_raw_reads']:
            r = overlap(f, r, pairtig_dir, minPairtigLen=config.data['minimum_pairtig_length'],
                minOverlap=config.data['pair_overlap_length'],
                threads=config.data['threads'])
            panda_pool_results.append(r)
        #panda_pool = multiprocessing.Pool(processes=panda_processes)
        #panda_pool_results = [panda_pool.apply_async(overlap, (f, r, pairtig_dir),
        #    dict(minPairtigLen=config.data['minimum_pairtig_length'],
        #        minOverlap=config.data['pair_overlap_length']) ) for f, r in
        #        config.data['input_raw_reads']]

        #panda_pool.close()
        #panda_pool.join()
        for pairtig_file in panda_pool_results:
        #for result in panda_pool_results:
            #pairtig_file = result.get()
            with open(pairtig_file) as fp:
                reads_counts.append(count_reads_in_file(fp))
                #r = result.get()
            config.data['pairtig_read_files'].append(os.path.abspath(pairtig_file))
    return reads_counts


def extract_from_metagenome(read1, read2=None, outfile='./rrna', db=None):
    """Get reads from marker genes from a bulk metagenome
    """
    try:
        ndb = len(db)
    except:
        print "no SortMeRNA databases specified "

    if read2:
        # have to interleave the files
        smr_in = tempfile.NamedTemporaryFile(delete=False)
        with gzip.open(read1), gzip.open(read2) as fp1, fp2:
            r1 = amplishot.parse.fastx.FastxReader(fp1)
            r2 = amplishot.parse.fastx.FaxtxReader(fp2)
            for name1, seq1, qual1 in r1.parse():
                name2, seq2, qual2 = r2.parse()
                if qual1:
                    smr_in.write("@%s\n%s\n+\n%s\n" % (name1, seq1, qual1))
                    smr_in.write("@%s\n%s\n+\n%s\n" % (name2, seq2, qual2))
                else:
                    smr_in.write(">%s\n%s\n" % (name1, seq1))
                    smr_in.write(">%s\n%s\n" % (name2, seq2))
        smr_in.close()
        smr = sortmerna.Sortmerna()
        smr.Parameters['--pair-in'].on()
        smr.Parameters['--accept'].on(outfile)
        smr.Parameters['--I'].on(smr_in.name)
    else:
        smr_in = read1


def get_normalized_read_count(*args):
    ''' given a list of open file objects, counts the number of fasta/fastq
    records and return the smallest number rounded bound to the nearest 100
    '''
    c = []
    for fd in args:
        c.append( count_reads_in_file(fd))
    x = min(c)
    if len(c) == 1:
        return x
    else:
        normalized_read_count = x if x % 100 == 0 else x - 100 - x % 100
        return normalized_read_count

def profile_only(config):
    """Create a profile of the community without trying to perform an assembly
        Steps:
            1. Overlap reads (optional based on config)
            2. Map reads
            3. OTU table from mappping
    """
    root_dir = config.data['output_directory'] = os.path.abspath(config.data['output_directory'])

    normalized_read_count = pair_and_read_count(config, root_dir)
    indata = fixup_aliases(config)

    for readfile, samplename in indata:
        logging.info('processing %s...', readfile)
        sample_mapping_file = os.path.join(root_dir, samplename + '.initial_mapping.sam')
        bowtie_params = {
            '-x': config.data['mapper_database'],
            '-S': sample_mapping_file,
            '-p': config.data['threads'],
            #'-u': 10000,
            }
        if isinstance(readfile, list):
            bowtie_params['-1'] = readfile[0]
            bowtie_params['-2'] = readfile[1]
        else:
            bowtie_params['-U'] = readfile

        bowtie_results = map_to_reference( params=bowtie_params)

def pair_and_read_count(config, root_dir):
    # generate fragment consensus sequences - pandaseq
    read_counts = []
    if 'skip_pairtigs' not in config.data:
        read_counts = generate_pairtigs(config, root_dir)
        # take the smallest number of pairtigs and round down to the nearest 100
    else:
        for rrf in config.data['input_raw_reads']:
            if isinstance(rrf, list):
                infile = rrf[0]
            else:
                infile = rrf
            with gzip.open(infile) as fd:
                read_counts.append(count_reads_in_file(fd))

    if len(read_counts) > 1:
        x = min(read_counts)
        return x if x % 100 == 0 else x - 100 - x % 100
    else:
        return read_counts[0]


def fixup_aliases(config):
    if 'aliases' in config.data:
        if 'skip_pairtigs' not in config.data:
            if len(config.data['aliases']) != len(config.data['input_raw_reads']):
                logging.warn('There are a different number of aliases and input'\
                    'files. Aliases will not be used')
                indata = make_aliases(config)
            else:
                indata = zip(config.data['pairtig_read_files'], config.data['aliases'])
        else:
            indata = zip(config.data['input_raw_reads'], config.data['aliases'])
    else:
        indata = make_aliases(config)
        
    return indata


def segregate_and_assemble(config, indata, root_dir):
    # taxonomy assigner
    taxseg = amplishot.taxon_segregator.TaxonSegregator(config.data['taxonomy_file'],
            cutoffs=config.data['mapping_similarity_cutoffs'],
            neighboursfile=config.data['neighbours_file'])

    # assembly wrapper
    wrapper = assemble.AssemblyWrapper(config.data['assembly_method'], config,
            preAssembleReduction=config.data['preassemble_clustering'])
    for readfile, samplename in indata:
        logging.info('processing %s...', readfile)
        sample_mapping_file = os.path.join(root_dir, samplename + '.initial_mapping.sam')
        bowtie_params = {
            '-x': config.data['mapper_database'],
            '-S': sample_mapping_file,
            '-p': config.data['threads'],
            #'-u': 10000,
            }

        if isinstance(readfile, list):
            bowtie_params['-1'] = readfile[0]
            bowtie_params['-2'] = readfile[1]
        else:
            bowtie_params['-U'] = readfile

        bowtie_results = map_to_reference( params=bowtie_params)

        # partition dataset based on mapped reads
        logging.info('partitioning...')
        taxon_root = os.path.join(root_dir, samplename, 'root')
        taxseg.parse_sam2(open(sample_mapping_file))
        bowtie_results.cleanUp()

        output_fastq = False
        output_qual = True
        output_fasta = True
        output_pairs = False
        sep12 = False
        if config.data['assembly_method'] == 'spades' or config.data['assembly_method'] == 'ray':
            output_fastq = True
            output_qual = False
            output_fasta = False
            output_pairs = True
            sep12 = True

        good_taxons = taxseg.segregate2(root=taxon_root,
                minCount=config.data['taxon_coverage'][1],
                minCoverage=config.data['taxon_coverage'][0],
                sam=False, fastq=output_fastq, fasta=output_fasta,
                qual=output_qual, pairs=output_pairs, sep12=sep12)
        taxseg.clear()

        if len(good_taxons) < 1:
            logging.info("No taxons for assembly")
        else:
            logging.info('There are %i taxons suitable for assembly',
                    len(good_taxons))

            if good_taxons.has_key(()) and not config.data['assemble_unknowns']:
                del good_taxons[()]

            wrapper(good_taxons, samplename)

    return wrapper

def pipeline(config):
    """Run all the steps of amplishot
    """
    root_dir = config.data['output_directory'] = os.path.abspath(config.data['output_directory'])

    # generate fragment consensus sequences - pandaseq
    normalized_read_count = pair_and_read_count(config, root_dir)

    # map overlapped fragments with bowtie
    # create the file for all the full-length seqs
    full_length_seqs =\
    open(os.path.join(root_dir,
        config.data.get('reconstruced_seq_file', 'full_length_sequences.fa')), 'w')

    indata = fixup_aliases(config)

    wrapper = segregate_and_assemble(config, indata, root_dir)

    # check to see if there are any full length sequences
    # if not we finish here
    if len(wrapper.fullLengthSeqs) < 1:
        logging.warn("No 16S reconstructions could be made")
    else:
        # fix up the orientation of the full-length sequences to they are all in
        # the forward direction then write them to file
        logging.info("Orientating full-length sequences...")
        wrapper.fullLengthSeqs = orientate(wrapper.fullLengthSeqs)
        #if len(wrapper.fullLengthQuals):
        #    flq = open(full_length_seqs.name + '.qual', 'w')
        #    wrapper.write(full_length_seqs, qfp=flq)
        #    flq.close()
        #else:
        wrapper.write(full_length_seqs)

        full_length_seqs.close()
        # pick OTUs and representative set
        out_otu_map = amplishot.otu_table.pick_otus(full_length_seqs.name,
                outputFileName=os.path.join(root_dir, 'full_length_otus.txt'),
                similarity=config.data['otu_clustering_similarity'])
        rep_set = amplishot.otu_table.pick_rep_set(full_length_seqs.name, out_otu_map,
                outputFilePath=os.path.join(root_dir,
                    config.data.get('repset_output_file',
                        'full_length_sequences.repset.fa')))
        rep_set_seqs = dict()
        with open(rep_set) as fp:
            fxparser = amplishot.parse.fastx.FastxReader(fp)
            for name, seq, qual in fxparser.parse():
                rep_set_seqs[name] = seq

        logging.info('There are %i OTUs, assigning taxonomy...', len(rep_set_seqs))

        #assign taxonomy
        taxonomy_map = assign_taxonomy.assign_taxonomy(config.data['assign_taxonomy_method'],
                rep_set, config)

        observation_metadata = defaultdict(dict)
        score_field_name = 'confidence'
        
        if config.data['assign_taxonomy_method'] == 'blast':
            score_field_name = 'evalue'
        elif config.data['assign_taxonomy_method'] == 'bowtie':
            score_field_name = 'similarity'

        for otu, metadata in taxonomy_map.items():
            observation_metadata[otu]['taxonomy'] = metadata[0].split('; ')
            observation_metadata[otu][score_field_name] = metadata[1]

        logging.info('generating OTU table in biom format...')

        # create Bowtie database from representative set
        abundance = amplishot.otu_table.OTUTableGenerator(rep_set_seqs,
                root_dir)
        #generate abundance based on coverage and create OTU table
        for read_file, sample_name in indata:
            _params = {'-p': config.data['threads'], '-u': normalized_read_count}
            if isinstance(read_file, list):
                _params['-1'] = read_file[0]
                _params['-2'] = read_file[1]
            else:
                _params['-U'] = read_file
            abundance.generate_abundance(sample_name,
                    params=_params, tmpFile=False)

        abundance.generate_biom_table()
        abundance.add_metadata(observation_metadata=observation_metadata)
        with open(os.path.join(root_dir, 'abundances.biom'), 'w') as fp:
            fp.write(str(abundance))

        if config.data['normalize_otu_table']:
            abundance.normalize()
            with open(os.path.join(root_dir, 'abundances.normalized.biom'), 'w') as fp:
                fp.write(str(abundance))

    logging.info('Done.')


###############################################################################
###############################################################################
###############################################################################
###############################################################################

if __name__ == '__main__':
#-------------------
    # initialise the config parser
    config = AmplishotConfig()

#--------------------
    #place options that will be used for all subparsers here
    common_options_parser = argparse.ArgumentParser(add_help=False)
    common_options_parser.add_argument('-c', '--config-file', dest='config',
            #default=os.path.join(os.path.expanduser('~'),'.amplishot_config.yml'),
            help='specify a custom location to the amplishot configuration\
            file. By default all values stored in the config file will\
            be used unless they are overwritten by command-line options')
    common_options_parser.add_argument('--quiet', dest='logger_quiet',
            action='store_const', const='CRITICAL', help='change the log verbosity so that only'\
                    'the most critical messages are reported')
    common_options_parser.add_argument('--log-level', choices=['DEBUG', 'INFO',
        'WARNING', 'ERROR', 'CRITICAL'], dest='log_level',
        help='Set the log level')
    common_options_parser.add_argument('--log-file', dest='log_file',
            help='Change the destination of logging.')

#--------------------
    # create a threads parser specifically to hold that option, then we can
    # pass it to the parsers that need it
    threads_parser = argparse.ArgumentParser(add_help=False)
    threads_parser.add_argument('-t', '--threads', type=int, dest='threads',
             help='The number of threads to use')

#--------------------
    # top level parser for running the pipeline
    # amplishot [OPTIONS] <file>...
    parser = argparse.ArgumentParser(
            prog='amplishot',
            formatter_class=argparse.RawDescriptionHelpFormatter,
            description=textwrap.dedent("""\
            %(prog)s is a pipeline for reconstructing full length 16S sequences
            from specially prepared 'amplishot' libraries.

            There are two ways to run amplishot:
            1. First is to perform each step individually using the subcommands
            listed at the bottom of this help message.

            2. The other way is as a pipline, which will take the
            raw reads from the sequencer and give you back full-length 16S
            sequences.  If you want to use the pipline simply leave out the
            subcommand name and supply the options and files nessessary.  If
            you want to perform each step manually refer to the subcommand help
            by invoking '%(prog)s pipeline -h'
            """)
            )
    subparser = parser.add_subparsers(title='Commands',
            metavar=' ',
            dest='subparser_name')

#--------------------
    #pipeline parser
    # amplishot pipeline [OPTIONS] file [file...]
    pipeline_parser = subparser.add_parser('pipeline',
            help='Run %(prog)s from start to finish',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            parents=[common_options_parser, threads_parser]
            )
    pipeline_parser.add_argument('-T','--taxonomy', #required=True,
    dest='taxonomy_file', help="File\
            containing the taxonomy mapping of the reference sequences in the\
                    bowtie database")
    pipeline_parser.add_argument('-d', '--bowtie-database', #required=True,
    dest='mapper_database', help="the\
            path to the bowtie database, without the suffixes; just like you\
            would write when using bowtie")
    pipeline_parser.add_argument('-o',
            '--output-directory', dest='output_directory',
            help='Root directoy under which all files and folders from the\
            pipeline will be generated from')
    #pipeline_parser.add_argument('-m', '--minscore', dest='phrap_minscore',
    #        type=int, default=config.data.get('phrap_minscore', 300),
    #        help='The minimum alignment score used in\
    #        the phrap assembly')
    pipeline_parser.add_argument('-s', '--similarity', type=float,
            dest='otu_clustering_similarity',
            help='The similarity'\
                    ' required between sequences AFTER reconstruction'\
                    ' for OTU clustering')
    pipeline_parser.add_argument('-i', '--input', nargs=2, action='append',
            help='paths to first and pecond pairs of reads for \'pairtig\'\
            generation.  Can be specified multiple times for many samples',
            dest='input_raw_reads')
    pipeline_parser.add_argument('-l', '--min-length',
            dest='minimum_pairtig_length',
            help='The minimal acceptable length for\
            assembled pairs')
    pipeline_parser.add_argument('-O', '--overlap-length',
            dest='pair_overlap_length',
            help='the minimum length that pairs must overlap by')
    pipeline_parser.set_defaults(func=pipeline)

    args = parser.parse_args()

    write_new_config = config.check_config_and_set_output(args)
#-------------------
    # set up the logger
    if args.logger_quiet is not None:
        config.data['log_level'] = args.logger_quiet

    numeric_level = getattr(logging, config.data['log_level'], None)
    if not isinstance(numeric_level, int):
        raise ValueError('Invalid log level: %s' % config.data['log_level'])

    logging.basicConfig(level=numeric_level,
            format='%(levelname)s\t%(asctime)s\t%(message)s')

    if config.data['log_file'] is not None:
        logging.basicConfig(filename=config.data['log_file'])

    args.func(config)
    if write_new_config:
        write_config(config)


###############################################################################
###############################################################################
###############################################################################
###############################################################################
