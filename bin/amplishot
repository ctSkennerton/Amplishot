#!/usr/bin/python
###############################################################################
#
# amplishot.py - Pipeline for generating full-length 16S sequences and performing
#                community abundance measurements on the output#
###############################################################################
#                                                                             #
#    This program is free software: you can redistribute it and/or modify     #
#    it under the terms of the GNU General Public License as published by     #
#    the Free Software Foundation, either version 3 of the License, or        #
#    (at your option) any later version.                                      #
#                                                                             #
#    This program is distributed in the hope that it will be useful,          #
#    but WITHOUT ANY WARRANTY; without even the implied warranty of           #
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the            #
#    GNU General Public License for more details.                             #
#                                                                             #
#    You should have received a copy of the GNU General Public License        #
#    along with this program. If not, see <http://www.gnu.org/licenses/>.     #
#                                                                             #
###############################################################################

__author__ = "Connor Skennerton"
__copyright__ = "Copyright 2012-2013"
__credits__ = ["Connor Skennerton"]
__license__ = "GPL3"
__version__ = "0.0.1"
__maintainer__ = "Connor Skennerton"
__email__ = "c.skennerton@gmail.com"
__status__ = "Development"

###############################################################################

import argparse
import sys
import tempfile
import os
import shutil
import subprocess
import textwrap

import amplishot.parse.fastx 
from amplishot import taxon_segregator

###############################################################################
###############################################################################
###############################################################################
###############################################################################

###############################################################################
###############################################################################
###############################################################################
###############################################################################
def panda(args):
    panda = pandaseq.Pandaseq()
    panda.Parameters['-o'].on(args.olap)
    panda.Parameters['-l'].on(args.minlen)
    panda.Parameters['-f'].on(args.read_1)  
    panda.Parameters['-r'].on(args.read_2)
    return panda()

#def make_partition_files(names, directory=None):
#    ret = dict()
#    outdirs = list()
#    for i in names:
#        try:
#            shutil.rmtree(os.path.join(directory,i))
#        except:
#            pass
#        finally:
#            os.mkdir(os.path.join(directory,i))
#        outdirs.append(os.path.join(directory,i))
#        ret[i] = open(os.path.join(directory,i,'in.fa'), 'w')
#        ret[i+'qual'] = open(os.path.join(directory,i,'in.fa.qual'), 'w')
#
#    try:
#        shutil.rmtree(os.path.join(directory,'nomatch'))
#    except:
#        pass
#    finally:
#        os.mkdir(os.path.join(directory,'nomatch'))
#        
#    outdirs.append(os.path.join(directory,'nomatch'))
#    ret['nomatch'] = open(os.path.join(directory, 'nomatch','in.fa'), 'w')
#    ret['nomatchqual'] = open(os.path.join(directory, 'nomatch','in.fa.qual'), 'w')
#        
#    return ret, outdirs

def map_to_reference(inreads, bowtiedb, threads):
    """Take overlapped reads from pandaseq and map them to reference sequences
       returns path to output file containing sam formated alignments
       inreads: path to file containing overlapped reads from pandaseq
       bowtiedb: path to fileprefix of the bowtie database
    """
    sam = tempfile.NamedTemporaryFile(suffix='.sam', delete=False)
    with open(os.devnull, 'w') as fnull:
        subprocess.call(['bowtie2', '-U',inreads, '-x', bowtiedb, '-p',
            str(threads)], stdout = sam,
            stderr = fnull)
    
    sam.close()
    return sam.name

def taxon_partition(samfile, taxonomyFile, rootdir='root'):
    """Call the taxon segregator and return a list of tuples of taxonomies that
       have adequate coverage for assembly
    """
    taxseg = amplishot.taxon_segregator.TaxonSegregator(taxonomyFile)
    taxseg.parse_sam(samfile)
    return taxseg.segregate(root=rootdir)

#def partition(infp, tempdir=None):
#    """split the dataset based on the presence of conserved marker genes
#       infile - input file object
#       partitionFiles - a dictionary containing the file names for outputing
#       partitioned fasta reads
#       tempdir - output directory for partitions, if not set makes a tempdir
#    """
#    if tempdir is None:
#        tempdir = tempfile.mkdtemp()
#
#    tcs = conserved_sequences.ConservedSequences()
#    outfiles, outdirs = make_partition_files(tcs.positions, tempdir)
#
#    seqs = tcs.conserved_sequences.keys()
#    wu = wumanber.WuManber(seqs)
#    count = 0
#    total_count = 0
#    if isinstance(infp, str):
#        infp = open(infp)
#    
#    for name, seq, qual in MinimalFastqParser(infp, strict=False):
#        total_count += 1
#        qual = map(lambda x: ord(x)-33, qual)
#        qual_str = ' '.join(map(str,qual))
#        ret = wu.search_text(str(seq))
#        if ret is not None:
#            count += 1
#            # write the fasta sequence file
#            outfiles[str(tcs.conserved_sequences[ret[1]].pos)].write('>%s\n%s\n' % (name, seq))
#            # write the fasta qual file
#            outfiles[str(tcs.conserved_sequences[ret[1]].pos)+'qual'].write('>%s\n%s\n'
#                    % (name, qual_str))
#        else:
#            outfiles['nomatch'].write('>%s\n%s\n' % (name, seq))
#            outfiles['nomatchqual'].write('>%s\n%s\n' % (name, qual_str))
#
#    for f in outfiles.values():
#        f.close()
#
#    if count == total_count:
#        shutil.rmtree(os.path.join(tempdir,'nomatch'))
#        outdirs.remove(os.path.join(tempdir,'nomatch'))
#
#    print 'Able to Partition %i (%.2f %%) reads into %i bins' % (count,
#            (count / total_count)*100, len(outdirs) - 1 )
#    return outdirs

def cd_hit_reduce(workingdir, infile='in.fa', outfile='cdhitout.fa'):
    #print ' '.join(["cd-hit-est","-c","0.98", "-M", "1000","-o", os.path.join(workingdir,outfile),"-i",os.path.join(workingdir,infile)])
    proc = subprocess.Popen(["cd-hit-est","-c","0.98", "-M", "1000","-o",
                os.path.join(workingdir,outfile),"-i",os.path.join(workingdir,infile)],
            stdout=subprocess.PIPE, stderr=subprocess.PIPE)

    stdout, stderr = proc.communicate()
    return_value = proc.returncode
    return stdout, stderr, return_value

#def extract_subset(headers, dbfp, parser=MinimalFastaParser, outfp=None, nameTransform=None):
#    """nameTransform is a function to modify the names of the output
#    """
#    return_dict = dict()
#    for name, seq in parser(dbfp):
#        if name in headers:
#            if nameTransform is None:
#                if outfp is None:
#                    return_dict[name] = seq
#                else:
#                    outfp.write('>%s\n%s\n' % (name, seq))
#            else:
#                if outfp is None:
#                    return_dict[nameTransform(name)] = seq
#                else:
#                    outfp.write('>%s\n%s\n' % (nameTransform(name), seq))
#    return return_dict
#
#
def process_cd_hit_results(directory, cdhitout = 'cdhitout.fa', cdhitin = 'in.fa'):
    clustered_qual = open(os.path.join(directory,cdhitout+'.qual'), 'w')
    clustered_reads = set()
    cd_hit_fasta = open(os.path.join(directory,cdhitout))
    fxparser = amplishot.parse.fastx.FastxReader(cd_hit_fasta)
    for name, seq, qual in fxparser.parse():
        clustered_reads.add(name)

    cd_hit_fasta.close()
    inqual = open(os.path.join(directory,cdhitin+'.qual'))
    qualparser = amplishot.parse.fastx.QualityReader(inqual)
    for name, qual in qualparser.parse():
        if name in clustered_reads:
            clustered_qual.write('>%s\n%s\n' % (name, qual))
    inqual.close()


def phrap_assemble(workingdir, infile='cdhitout.fa', suppressStdout=True, suppressStderr=True):
    out = None
    err = None
    if suppressStdout:
        out = open(os.devnull, "w")
    if suppressStderr:
        err = open(os.devnull, "w")
    print ' '.join(['phrap',infile,'-minscore','300'])
    return subprocess.call(['phrap',infile,'-minscore','300'],
            stderr=err, stdout=out, cwd=workingdir)
def demultiplex(args): pass
def assemble(args): pass
def overlap(args): pass
def map_reads(args): pass
def filter_sam(args): pass
def cluster(args): pass
def pipeline(args):
    """Run all the steps of amplishot
    """
    root_dir = args.outdir
    taxon_root = os.path.join(root_dir, 'root')
    # generate fragment consensus sequences - pandaseq
    #panda_out = panda(args)

    # map overlapped fragments with bowtie
    samfile = map_to_reference(args.infile, args.bowtiedb, args.threads)

    # partition dataset based on mapped reads
    good_taxons, bad_taxons = taxon_partition(samfile, args.taxonomy,
            rootdir=taxon_root)
    print 'There are %i taxons suitable for assembly and %i taxons with\
            incomplete coverage' % (len(good_taxons), len(bad_taxons))
    if len(good_taxons) < 1:
        print "No taxons for assembly"
        sys.exit(1)

    good_taxons = [os.path.join(taxon_root, *x) for x in good_taxons]
    
    # dataset reduction - cd-hit - each partition - threaded
    for t in good_taxons:
        cd_hit_reduce(t, infile='reads.fa')

    for t in good_taxons:
        process_cd_hit_results(t, cdhitin='reads.fa')

    ## generate overlaps - phrap - each partition - threaded
    for t in good_taxons:
        phrap_assemble(t)
    #chimera checking - uchime, decipher, chimeraslayer

    #read alignment - bwa

    #generate abundance based on coverage

    #assign taxonomy

    #generte OTU table

###############################################################################
###############################################################################
###############################################################################
###############################################################################

if __name__ == '__main__':
   
#--------------------
    #place options that will be used for all subparsers here
    common_options_parser = argparse.ArgumentParser(add_help=False)
    common_options_parser.add_argument('-c', '--config-file', dest='config',
            default=os.path.join(os.path.expanduser('~'),'.amplishot_config.yml'),
            help='specify a custom location to the amplishot configuration\
            file. By default all values stored in the config file will\
            be used unless they are overwritten by command-line options\
            or if the --suppress-config-file option is set')
    common_options_parser.add_argument('--supress-config-file',
    action='store_true', default=False, help='Do not use any values stored in\
            the config file')

#--------------------
    # create a threads parser specifically to hold that option, then we can
    # pass it to the parsers that need it
    threads_parser = argparse.ArgumentParser(add_help=False)
    threads_parser.add_argument('-t', '--threads', type=int, dest='threads',
            default=1, help='The number of threads to use')

#--------------------
    # top level parser for running the pipeline
    # amplishot [OPTIONS] <file>...
    parser = argparse.ArgumentParser(
            prog='amplishot',
            formatter_class=argparse.RawDescriptionHelpFormatter,
            description=textwrap.dedent("""\
            %(prog)s is a pipeline for reconstructing full length 16S sequences 
            from specially prepared 'amplishot' libraries.
            
            There are two ways to run amplishot:
            1. First is to perform each step individually using the subcommands 
            listed at the bottom of this help message.
            
            2. The other way is as a pipline, which will take the
            raw reads from the sequencer and give you back full-length 16S
            sequences.  If you want to use the pipline simply leave out the
            subcommand name and supply the options and files nessessary.  If
            you want to perform each step manually refer to the subcommand help
            by invoking '%(prog)s pipeline -h'
            """)
            )
    subparser = parser.add_subparsers(title='Commands',
            metavar=' ',
            dest='subparser_name')

#--------------------
    #pipeline parser
    # amplishot pipeline [OPTIONS] file [file...]
    pipeline_parser = subparser.add_parser('pipeline',
            help='Run %(prog)s from start to finish',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            parents=[common_options_parser]
            )
    pipeline_parser.add_argument('-T','--taxonomy', required=True, dest='taxonomy', help="File\
            containing the taxonomy mapping of the reference sequences in the\
                    bowtie database")
    pipeline_parser.add_argument('-d', '--bowtie-database', required=True, dest='bowtiedb', help="the\
            path to the bowtie database, without the suffixes; just like you\
            would write when using bowtie")
    pipeline_parser.add_argument('-o', '--output-directory',dest='outdir', default='.',
            help='Root directoy under which all files and folders from the\
            pipeline will be generated from')
    pipeline_parser.add_argument('file',nargs='+',
            help='Input files containing read pairs')
    pipeline_parser.set_defaults(func=pipeline)



#--------------------
    # demultiplexing parser
    # amplishot demultiplex [OPTIONS] {<file1> <file2>}...
    demultiplex_parser = subparser.add_parser('demultiplex', 
            help='Split datasets based on user defined barcode sequences', 
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            parents=[common_options_parser]
            )
    demultiplex_parser.add_argument('-o', '--output-directory', default='.', dest='outdir',
            help='Specify an output directory for demutiplexed files')
    demultiplex_parser.add_argument('-b', '--barcode-mapping', dest='barcodes',
            required=True, help="""A file containing barcodes to search for.
            The file must be tab separated with each barcode on a single line.
            The first column should be an identifier for the barcode, the
            second column should be the sequence of the barcode.  If the read 
            pair is dual barcoded then a third column can be
            specified as the barcode for the reverse read.  Note that both
            barcodes should be in the correct orientation for their read (i.e.
            the second barcode should be reversed complemented)
            """)
    demultiplex_parser.add_argument('file', nargs='+', 
            help='Input file(s) that need to be demultiplexed they may be\
                    in fasta, fastq and be gziped')
    demultiplex_parser.set_defaults(func=demultiplex)

#--------------------
    # overlap parser
    # amplishot overlap [OPTIONS] {<file1> <file2>}...
    overlap_parser = subparser.add_parser('overlap', 
            help='overlap read pairs to generate a consensus sequence',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            parents=[common_options_parser, threads_parser]
            )
    overlap_parser.add_argument('-o', '--output-directory',
            default='./overlapped', dest='outdir', 
            help='Output directory for the consensus fragments')
    overlap_parser.add_argument('-l', '--min-length', default=350,
            dest='minlength', help='The minimal acceptable length for\
            assembled pairs')
    overlap_parser.add_argument('-O', '--overlap-length', default=30,
            dest='overlap', 
            help='the minimum length that pairs must overlap by')
    overlap_parser.add_argument('file', nargs='+',
            help='Input must be pairs of files that contain the forward\
            and reverse reads for assembly into consensus fragments')
    overlap_parser.set_defaults(func=overlap)

#--------------------
    # mapping parser
    # amplishot map [OPTIONS] <file>...
    map_parser = subparser.add_parser('map',
            help='map assembled pairs to a reference database',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            parents=[common_options_parser, threads_parser]
            )
    map_parser.add_argument('-x', '--index', dest='db', required=True,
            help='Path to the index file conatining the 16S database.\
            Must be in bowtie2 format')
    map_parser.add_argument('-o', '--out-sam', dest='sam', required=True,
            help='output file in sam format for alignments')
    map_parser.add_argument('file', nargs='+', 
            help='input files containing assembled fragments')
    map_parser.set_defaults(func=map_reads)

#--------------------
    # filtering parser
    # amplishot filter [OPTIONS] <file.sam>...
    filter_parser = subparser.add_parser('filter',
            help='segregate reads based on the similarity and taxonomy\
            of reference sequences', 
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            parents=[common_options_parser]
            )
    filter_parser.add_argument('-T', '--taxonomy-file', required=True,
            dest='taxonomy', help='A mapping file in either Greengenes or\
            SILVA format that maps the reference IDs to their taxonomies')
    filter_parser.add_argument('-p', '--percent-id', type=float, default=0.98,
            dest='percentid', 
            help='The global percentage Identity that a read must have to the\
            reference sequence. A good rule of thumb is to set this value to\
            be 1% lower than the clustering percentage used on the reference\
            database.')
    filter_parser.add_argument('-C', '--taxon-depth', type=int, default=2,
            dest='taxon_depth',
            help='After taxonomic filtering, this value dictates the minimum\
            coverage for EVERY covered base in the references of that taxon\
            must contain to be considered suitable for assembly')
    filter_parser.add_argument('-m', '--minimum-bases', type=int, default=1000,
            dest='taxon_width',
            help='After taxonomic filtering, this value dictates the minimum\
            number of bases that must have query bases mapping to them for\
            the entire taxon')
    filter_parser.add_argument('sam', nargs='+', 
            help='sam formatted files containing assembled fragments for\
            taxonomic filtering')
    filter_parser.set_defaults(func=filter_sam)

#--------------------
    #clustering parser
    # amplishot cluster [OPTIONS] <file>...
    cluster_parser = subparser.add_parser('cluster', 
            help='Remove redundant coverage in taxons.  Greating decreases\
            assembly time',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            parents=[common_options_parser]
            )
    cluster_parser.add_argument('-p', '--percent-identity', type=float,
            default=0.98, dest='percentid', 
            help='The percent identity that two reads must have to be clustered')
    cluster_parser.add_argument('file', nargs='+', 
            help='fasta files containing reads to be clustered')
    cluster_parser.set_defaults(func=cluster)

#--------------------
    # assembly parser
    # amplishot assemble [OPTIONS] <file.fa>...
    assembly_parser = subparser.add_parser('assemble',
            help='Assemble individual taxons into full-length 16S sequences',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            parents=[common_options_parser]
            )
    assembly_parser.add_argument('-m', '--min-score', dest='minscore',
            type=int, default=300,
            help='The minimum alignment score used in phrap')
    assembly_parser.add_argument('file', nargs='+', 
            help='Path to files containing taxonomies for assembly')
    assembly_parser.set_defaults(func=assemble)

    args = parser.parse_args()        
    args.func(args) 


###############################################################################
###############################################################################
###############################################################################
###############################################################################
