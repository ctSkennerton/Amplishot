#!/usr/bin/python
###############################################################################
#
# amplishot.py - Pipeline for generating full-length 16S sequences and
#                performing community abundance measurements on the output
#
###############################################################################
#                                                                             #
#    This program is free software: you can redistribute it and/or modify     #
#    it under the terms of the GNU General Public License as published by     #
#    the Free Software Foundation, either version 3 of the License, or        #
#    (at your option) any later version.                                      #
#                                                                             #
#    This program is distributed in the hope that it will be useful,          #
#    but WITHOUT ANY WARRANTY; without even the implied warranty of           #
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the            #
#    GNU General Public License for more details.                             #
#                                                                             #
#    You should have received a copy of the GNU General Public License        #
#    along with this program. If not, see <http://www.gnu.org/licenses/>.     #
#                                                                             #
###############################################################################

__author__ = "Connor Skennerton"
__copyright__ = "Copyright 2012-2013"
__credits__ = ["Connor Skennerton"]
__license__ = "GPL3"
__version__ = "0.1.2"
__maintainer__ = "Connor Skennerton"
__email__ = "c.skennerton@gmail.com"
__status__ = "Development"

###############################################################################
import logging
import argparse
import sys
import tempfile
import os
import shutil
import subprocess
import textwrap
import gzip
from collections import defaultdict
import amplishot.parse.fastx
from amplishot import taxon_segregator, otu_table, assign_taxonomy
from amplishot.app import bowtie_pycogent, phrap_pycogent, pandaseq_pycogent
from amplishot.config import AmplishotConfig
from cogent.app.cd_hit import CD_HIT_EST
from cogent.app.util import FilePath
###############################################################################

def map_to_reference(params=None, stderr=None,
        stdout=None):
    """Take overlapped reads from pandaseq and map them to reference sequences
       returns path to output file containing sam formated alignments
       inreads: path to file containing overlapped reads from pandaseq
       bowtiedb: path to fileprefix of the bowtie database
    """
    b = amplishot.app.bowtie_pycogent.Bowtie2(params=params, SuppressStderr=True,
            SuppressStdout=True)
    logging.info(str(b))
    return b(stderr=stderr, stdout=stdout)

def taxon_partition(samfile, taxonomyFile, rootdir='root'):
    """Call the taxon segregator and return a list of tuples of taxonomies that
       have adequate coverage for assembly
    """
    taxseg = amplishot.taxon_segregator.TaxonSegregator(taxonomyFile)
    taxseg.parse_sam(samfile)
    return taxseg.segregate(root=rootdir)


def cd_hit_reduce(workingdir, infile='in.fa', outfile='cdhitout.fa', similarity=0.98, maxMemory=1000):
    cdhit = CD_HIT_EST(WorkingDir=workingdir)
    cdhit.Parameters['-i'].on(infile)
    cdhit.Parameters['-o'].on(outfile)
    cdhit.Parameters['-c'].on(similarity)
    cdhit.Parameters['-M'].on(maxMemory)
    cdhit()


def process_cd_hit_results(directory, cdhitout='cdhitout.fa', cdhitin='in.fa'):
    clustered_qual = open(os.path.join(directory,cdhitout+'.qual'), 'w')
    clustered_reads = set()
    cd_hit_fasta = open(os.path.join(directory,cdhitout))
    fxparser = amplishot.parse.fastx.FastxReader(cd_hit_fasta)
    for name, seq, qual in fxparser.parse():
        clustered_reads.add(name)

    cd_hit_fasta.close()
    inqual = open(os.path.join(directory,cdhitin+'.qual'))
    qualparser = amplishot.parse.fastx.QualityReader(inqual)
    for name, qual in qualparser.parse():
        if name in clustered_reads:
            clustered_qual.write('>%s\n%s\n' % (name, qual))
    inqual.close()


def phrap_assemble(workingdir, infile='cdhitout.fa',
        suppressStdout=True, suppressStderr=True, minscore=300):
    p = amplishot.app.phrap_pycogent.Phrap(SuppressStdout=suppressStdout,
            SuppressStderr=suppressStderr, WorkingDir=workingdir,
            PrependPositionals=True)
    p.Parameters['-minscore'].on(300)
    p.Parameters['-ace'].on()
    p.add_positional_argument(FilePath(infile))
    logging.debug('%s (wd: %s)', str(p), workingdir)

    return p()

def demultiplex(args):
    pass
def assemble(args):
    pass


def overlap(forward, reverse, minPairtigLen=350, minOverlap=30, stdout=None,
        stderr=None):
    panda = pandaseq_pycogent.Pandaseq()
    panda.Parameters['-f'].on(forward)
    panda.Parameters['-r'].on(reverse)
    panda.Parameters['-l'].on(minPairtigLen)
    panda.Parameters['-o'].on(minOverlap)
    panda.Parameters['-F'].on()
    return panda(stdout=stdout, stderr=stderr)


def map_reads(args):
    pass

def filter_sam(args): 
    taxseg = amplishot.taxon_segregator.TaxonSegregator(args.taxonomy)
    for samfile in args.sam:
        taxseg.parse_sam(samfile, args.percentid)
    good_taxons, bad_taxons = taxseg.segregate(root=args.outdir, minCount=args.taxon_width,
            minCoverage=args.taxon_depth)
    

def cluster(args): pass
def pipeline(config):
    """Run all the steps of amplishot
    """
    root_dir = config.data['output_directory']
    if not os.path.exists(root_dir):
        os.makedirs(root_dir)

    # generate fragment consensus sequences - pandaseq
    pairtig_dir = os.path.join(root_dir, 'pairtigs')
    if not os.path.exists(pairtig_dir):
        os.mkdir(pairtig_dir)

    for forward, reverse in config.data['input_raw_reads']:
        #forward = config.data['input_raw_reads'][i][0]
        #reverse = config.data['input_raw_reads'][i][1]
        logging.info('generating pairtigs for %s %s', forward, reverse)
        pairtig_file = os.path.join(pairtig_dir,
            os.path.splitext(os.path.basename(forward))[0] + '.pairtigs.fq')
        with open(pairtig_file, 'wb') as fp:
            panda_out = overlap(forward, reverse,
                    config.data['minimum_pairtig_length'],
                    config.data['pair_overlap_length'], stdout=fp)
        config.data['pairtig_read_files'].append(pairtig_file)

    # map overlapped fragments with bowtie
    # create the file for all the full-length seqs
    full_length_seqs =\
    open(os.path.join(root_dir, 
        config.data.get('reconstruced_seq_file', 'full_length_sequences.fa')), 'w')

    # taxonomy assigner
    taxseg = amplishot.taxon_segregator.TaxonSegregator(config.data['taxonomy_file'])
    
    # fix up the aliases
    indata = []
    if config.data['aliases']:
        indata = zip(config.data['pairtig_read_files'], config.data['aliases'])
    else:
        #logging.debug(config.data['pairtig_read_files'])
        for readfile in config.data['pairtig_read_files']:
            sample_file_name =\
            os.path.splitext(os.path.basename(readfile))[0]
            indata.append((readfile, sample_file_name))
    
    for readfile, samplename in indata:
        logging.info('processing %s...', readfile)
        sample_mapping_file = os.path.join(root_dir, samplename + '.initial_mapping.sam')
        map_to_reference( params={'-U': readfile,
            '-x': config.data['mapper_database'],
            '-S': sample_mapping_file,
            '-p': config.data['threads']})

        # partition dataset based on mapped reads
        logging.info('partitioning...')
        taxon_root = os.path.join(root_dir, samplename, 'root')
        taxseg.parse_sam(open(sample_mapping_file))
        good_taxons, bad_taxons = taxseg.segregate(root=taxon_root, 
                minCount=config.data['taxon_coverage'][1], 
                minCoverage=config.data['taxon_coverage'][0],
                sam=True)
        taxseg.clear()

        if len(good_taxons) < 1:
            logging.info("No taxons for assembly")
        else:
            logging.info('There are %i taxons suitable for assembly'\
                    ' and %i taxons with incomplete coverage',
                    len(good_taxons), len(bad_taxons))
            if config.data['assemble_unknowns']:
                good_taxons = [os.path.join(taxon_root, *x) for x in good_taxons]
            else:
                good_taxons = [os.path.join(taxon_root, *x) for x in good_taxons if x != ()]
            
            # dataset reduction - cd-hit - each partition
            logging.info('clustering...')
            for t in good_taxons:
                cd_hit_reduce(t, infile='reads.fa', 
                    similarity=config.data['read_clustering_similarity'],
                    maxMemory=config.data['cdhit_max_memory'])
            
            logging.info('assembling...')
            for t in good_taxons:
                process_cd_hit_results(t, cdhitin='reads.fa')

            ## generate overlaps - phrap - each partition
            full_length_counter = 1
            for t in good_taxons:
                results = phrap_assemble(t,
                        minscore=config.data['phrap_minscore'])
                # collect all full-length sequences
                fxparser = amplishot.parse.fastx.FastxReader(results['contigs'])
                for name, seq, qual in fxparser.parse(callback=amplishot.parse.fastx.greater_than, 
                length=config.data['minimum_reconstruction_length']):
                    full_length_seqs.write('>%s_%i %s\n%s\n' %
                            (sample_file_name, full_length_counter, name, seq))
                    full_length_counter += 1

    full_length_seqs.close()
    # pick OTUs and representative set
    out_otu_map = amplishot.otu_table.pick_otus(full_length_seqs.name,
            outputFileName=os.path.join(root_dir, 'full_length_otus.txt'),
            similarity=config.data['otu_clustering_similarity'])
    rep_set = amplishot.otu_table.pick_rep_set(full_length_seqs.name, out_otu_map,
            outputFilePath=os.path.join(root_dir, config.data['repset_output_file']))
    rep_set_seqs = dict()
    with open(rep_set) as fp:
        fxparser = amplishot.parse.fastx.FastxReader(fp)
        for name, seq, qual in fxparser.parse():
            rep_set_seqs[name] = seq

    logging.info('There are %i OTUs, assigning taxonomy...', len(rep_set_seqs))
    
    #assign taxonomy
    taxon_mapper = amplishot.taxon_segregator.TaxonSegregator(config.data['taxonomy_file'])
    with open(os.path.join(root_dir, 'full_length_alignments.sam'), 'w+b') as fp:
        map_to_reference(params={'-U': rep_set,
            '-x': config.data['mapper_database'],
            '-p': config.data['threads'], '-f': True}, stdout=fp)
        fp.seek(0)
        taxon_mapper.parse_sam(fp, percentId=config.data['minimum_taxon_similarity'])

    observation_metadata = defaultdict(dict)  #[{} for i in range(len(abundance.seqs))]
    for taxon, otu_representatives in taxon_mapper.taxon_mapping.items():
        gg_tax =\
        amplishot.assign_taxonomy.greengenes_format(taxon)
        for otu in otu_representatives:
            observation_metadata[otu.qname]['taxonomy'] = gg_tax
            if otu.is_unmapped():
                observation_metadata[otu.qname]['percent identity'] = 0.0
            else:
                observation_metadata[otu.qname]['percent identity'] =\
                otu.percent_identity()
            observation_metadata[otu.qname]['cigar'] = otu.cigar

    logging.info('generating OTU table in biom format...')
    
    # create Bowtie database from representative set
    abundance = amplishot.otu_table.OTUTableGenerator(rep_set_seqs,
            root_dir)
    #generate abundance based on coverage and create OTU table
    for read_file, sample_name in indata:
        abundance.generate_abundance(read_file, alias=sample_name)
    
    abundance.generate_biom_table()
    abundance.add_metadata(observation_metadata=observation_metadata)
    with open(os.path.join(root_dir, 'abundances.biom'), 'w') as fp:
        fp.write(str(abundance))
    
    if config.data['normalize_otu_table']:
        abundance.normalize()
        with open(os.path.join(root_dir, 'abundances.normalized.biom'), 'w') as fp:
            fp.write(str(abundance))

    logging.info('Done.')

###############################################################################
###############################################################################
###############################################################################
###############################################################################

if __name__ == '__main__':
#-------------------
    # initialise the config parser
    config = AmplishotConfig()

#--------------------
    #place options that will be used for all subparsers here
    common_options_parser = argparse.ArgumentParser(add_help=False)
    common_options_parser.add_argument('-c', '--config-file', dest='config',
            #default=os.path.join(os.path.expanduser('~'),'.amplishot_config.yml'),
            help='specify a custom location to the amplishot configuration\
            file. By default all values stored in the config file will\
            be used unless they are overwritten by command-line options')
    common_options_parser.add_argument('--quiet', dest='logger_quiet',
            action='store_const', const='CRITICAL', help='change the log verbosity so that only'\
                    'the most critical messages are reported')
    common_options_parser.add_argument('--log-level', choices=['DEBUG', 'INFO',
        'WARNING', 'ERROR', 'CRITICAL'], dest='log_level',
        default=config.data.get('log_level', 'INFO'),
        help='Set the log level')
    common_options_parser.add_argument('--log-file', dest='log_file',
            help='Change the destination of logging.')

#--------------------
    # create a threads parser specifically to hold that option, then we can
    # pass it to the parsers that need it
    threads_parser = argparse.ArgumentParser(add_help=False)
    threads_parser.add_argument('-t', '--threads', type=int, dest='threads',
            default=config.data.get('threads', 1), help='The number of threads to use')

#--------------------
    # top level parser for running the pipeline
    # amplishot [OPTIONS] <file>...
    parser = argparse.ArgumentParser(
            prog='amplishot',
            formatter_class=argparse.RawDescriptionHelpFormatter,
            description=textwrap.dedent("""\
            %(prog)s is a pipeline for reconstructing full length 16S sequences 
            from specially prepared 'amplishot' libraries.
            
            There are two ways to run amplishot:
            1. First is to perform each step individually using the subcommands 
            listed at the bottom of this help message.
            
            2. The other way is as a pipline, which will take the
            raw reads from the sequencer and give you back full-length 16S
            sequences.  If you want to use the pipline simply leave out the
            subcommand name and supply the options and files nessessary.  If
            you want to perform each step manually refer to the subcommand help
            by invoking '%(prog)s pipeline -h'
            """)
            )
    subparser = parser.add_subparsers(title='Commands',
            metavar=' ',
            dest='subparser_name')

#--------------------
    #pipeline parser
    # amplishot pipeline [OPTIONS] file [file...]
    pipeline_parser = subparser.add_parser('pipeline',
            help='Run %(prog)s from start to finish',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            parents=[common_options_parser, threads_parser]
            )
    pipeline_parser.add_argument('-T','--taxonomy', required=True,
    dest='taxonomy_file', help="File\
            containing the taxonomy mapping of the reference sequences in the\
                    bowtie database")
    pipeline_parser.add_argument('-d', '--bowtie-database', required=True,
    dest='mapper_database', help="the\
            path to the bowtie database, without the suffixes; just like you\
            would write when using bowtie")
    pipeline_parser.add_argument('-o',
            '--output-directory', dest='output_directory',
            default=config.data.get('output_directory', os.getcwd()),
            help='Root directoy under which all files and folders from the\
            pipeline will be generated from')
    pipeline_parser.add_argument('-m', '--minscore', dest='phrap_minscore',
            type=int, default=config.data.get('phrap_minscore', 300),
            help='The minimum alignment score used in\
            the phrap assembly')
    pipeline_parser.add_argument('-s', '--similarity', type=float,
            dest='otu_clustering_similarity',
            default=config.data.get('otu_clustering_similarity', 0.97),
            help='The similarity'\
                    ' required between sequences AFTER reconstruction'\
                    ' for OTU clustering')
    pipeline_parser.add_argument('-i', '--input', nargs=2, action='append',
            required=True,
            help='paths to first and pecond pairs of reads for \'pairtig\'\
            generation.  Can be specified multiple times for many samples',
            dest='input_raw_reads')
    pipeline_parser.add_argument('-l', '--min-length',
            default=config.data.get('minimum_pairtig_length', 350),
            dest='minimum_pairtig_length',
            help='The minimal acceptable length for\
            assembled pairs')
    pipeline_parser.add_argument('-O', '--overlap-length',
            default=config.data.get('pair_overlap_length', 30),
            dest='pair_overlap_length',
            help='the minimum length that pairs must overlap by')
    pipeline_parser.set_defaults(func=pipeline)



#--------------------
    # demultiplexing parser
    # amplishot demultiplex [OPTIONS] {<file1> <file2>}...
    #demultiplex_parser = subparser.add_parser('demultiplex', 
    #        help='Split datasets based on user defined barcode sequences', 
    #        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    #        parents=[common_options_parser]
    #        )
    #demultiplex_parser.add_argument('-o', '--output-directory', 
    #        default=config.data.get('output_directory', os.getcwd()), dest='output_directory',
    #        help='Specify an output directory for demutiplexed files')
    #demultiplex_parser.add_argument('-b', '--barcode-mapping', dest='barcodes',
    #        required=True, help="""A file containing barcodes to search for.
    #        The file must be tab separated with each barcode on a single line.
    #        The first column should be an identifier for the barcode, the
    #        second column should be the sequence of the barcode.  If the read 
    #        pair is dual barcoded then a third column can be
    #        specified as the barcode for the reverse read.  Note that both
    #        barcodes should be in the correct orientation for their read (i.e.
    #        the second barcode should be reversed complemented)
    #        """)
    #demultiplex_parser.add_argument('file', nargs='+', 
    #        help='Input file(s) that need to be demultiplexed they may be\
    #                in fasta, fastq and be gziped')
    #demultiplex_parser.set_defaults(func=demultiplex)

#--------------------
    # overlap parser
    # amplishot overlap [OPTIONS] {<file1> <file2>}...
    overlap_parser = subparser.add_parser('overlap', 
            help='overlap read pairs to generate a consensus sequence',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            parents=[common_options_parser, threads_parser]
            )
    overlap_parser.add_argument('-i', '--input', nargs=2, action='append',
            help='paths to first and pecond pairs of reads for \'pairtig\'\
            generation.  Can be specified multiple times for many samples',
            dest='input_raw_reads')
    overlap_parser.add_argument('-o', '--output-directory',
            default=config.data.get('output_directory', os.getcwd()), dest='output_directory', 
            help='Output directory for the consensus fragments')
    overlap_parser.add_argument('-l', '--min-length', default=config.data.get('minimum_pairtig_length', 350),
            dest='minimum_pairtig_length', help='The minimal acceptable length for\
            assembled pairs')
    overlap_parser.add_argument('-O', '--overlap-length', default=config.data.get('pair_overlap_length', 30),
            dest='pair_overlap_length', 
            help='the minimum length that pairs must overlap by')
    overlap_parser.set_defaults(func=overlap)

#--------------------
    # mapping parser
    # amplishot map [OPTIONS] <file>...
    map_parser = subparser.add_parser('map',
            help='map assembled pairs to a reference database',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            parents=[common_options_parser, threads_parser]
            )
    map_parser.add_argument('-i', '--input-reads', dest='pairtig_read_files',
            action='append', help='path to file containing \'pairtigs\'.  Can\
            be specified multiple times for multiple samples')
    map_parser.add_argument('-a', '--alias', dest='aliases', help='specify\
            an alias for the input file to give it a different name in the\
            resulting analyses')
    map_parser.add_argument('-x', '--index', dest='mapper_database',
            required=True, help='Path to the index file conatining'\
                    'the 16S database. Must be in bowtie2 format')
    map_parser.add_argument('-o', '--output-directory', dest='directory', 
            default=config.data.get('output_directory', os.getcwd()),
            help='Root directoy under which all files and folders from the\
            pipeline will be generated from')
    map_parser.set_defaults(func=map_reads)

#--------------------
    # filtering parser
    # amplishot filter [OPTIONS] <file.sam>...
    filter_parser = subparser.add_parser('filter',
            help='segregate reads based on the similarity and taxonomy\
            of reference sequences',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            parents=[common_options_parser]
            )
    filter_parser.add_argument('-o', '--output-directory',
            dest='filter_output_directory_prefix',
            default=config.data.get('output_directory', os.getcwd()), 
            help='output directory prefix')
    filter_parser.add_argument('-T', '--taxonomy-file', required=True,
            dest='taxonomy_file', help='A mapping file in either Greengenes or\
            SILVA format that maps the reference IDs to their taxonomies')
    filter_parser.add_argument('-p', '--percent-id', type=float, 
            default=config.data.get('initial_mapping_similarity', 0.98),
            dest='initial_mapping_similarity',
            help='''The global percentage Identity that a read must have to the\
            reference sequence. A good rule of thumb is to set this value to\
            be 1% lower than the clustering percentage used on the reference\
            database.''')
    filter_parser.add_argument('-C', '--taxon-coverage', nargs=2,
            default=config.data.get('taxon_coverage', [2, 1000]), dest='taxon_coverage',
            help='After taxonomic filtering, this value dictates the minimum\
            coverage and minimum nuber of bases that must be covered in that taxon\
            to be considered suitable for assembly.  Takes the form of two\
            numbered list: the first number is the minimum coverage and the\
            second number is the number of bases that need to be covered')
    filter_parser.set_defaults(func=filter_sam)

#--------------------
    #clustering parser
    # amplishot cluster [OPTIONS] <file>...
    cluster_parser = subparser.add_parser('reduce',
            help='Remove redundant coverage in taxons.  Greating decreases\
            assembly time',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            parents=[common_options_parser]
            )
    cluster_parser.add_argument('-p', '--percent-identity', type=float,
            default=config.data.get('read_clustering_similarity', 0.98), 
            dest='read_clustering_similarity',
            help='The percent identity that two reads must have to be clustered')
    cluster_parser.add_argument('-i', '--input-directory',
            dest='output_directory', default=config.data.get('output_directory', os.getcwd()))
    cluster_parser.add_argument('-m', '--cdhit-max-memory', default=config.data.get('cdhit_max_memory', 1000),
            dest='cdhit_max_memory')
    cluster_parser.set_defaults(func=cluster)

#--------------------
    # assembly parser
    # amplishot assemble [OPTIONS] <file.fa>...
    assembly_parser = subparser.add_parser('assemble',
            help='Assemble individual taxons into full-length 16S sequences',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            parents=[common_options_parser]
            )
    assembly_parser.add_argument('-m', '--min-score', dest='phrap_minscore',
            type=int, default=config.data.get('phrap_minscore', 300),
            help='The minimum alignment score used in phrap')
    assembly_parser.add_argument('-i', '--input-directory',
            dest='output_directory', help='The root directory\
            containing the samples reads split by taxonomy',
            default=config.data.get('output_directory', os.getcwd()))
    assembly_parser.set_defaults(func=assemble)
    
    
    args = parser.parse_args()
    config.populate_from_commandline(args)

#-------------------
    # set up the logger
    if args.logger_quiet is not None:
        config.data['log_level'] = args.logger_quiet

    numeric_level = getattr(logging, config.data['log_level'], None)
    if not isinstance(numeric_level, int):
        raise ValueError('Invalid log level: %s' % config.data['log_level'])

    logging.basicConfig(level=numeric_level,
            format='%(levelname)s\t%(asctime)s\t%(message)s')
    
    if config.data['log_file'] is not None:
        logging.basicConfig(filename=config.data['log_file'])

    args.func(config)


###############################################################################
###############################################################################
###############################################################################
###############################################################################
