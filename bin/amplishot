#!/usr/bin/python
###############################################################################
#
# amplishot.py - Pipeline for generating full-length 16S sequences and
#                performing community abundance measurements on the output
#
###############################################################################
#                                                                             #
#    This program is free software: you can redistribute it and/or modify     #
#    it under the terms of the GNU General Public License as published by     #
#    the Free Software Foundation, either version 3 of the License, or        #
#    (at your option) any later version.                                      #
#                                                                             #
#    This program is distributed in the hope that it will be useful,          #
#    but WITHOUT ANY WARRANTY; without even the implied warranty of           #
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the            #
#    GNU General Public License for more details.                             #
#                                                                             #
#    You should have received a copy of the GNU General Public License        #
#    along with this program. If not, see <http://www.gnu.org/licenses/>.     #
#                                                                             #
###############################################################################

__author__ = "Connor Skennerton"
__copyright__ = "Copyright 2012-2013"
__credits__ = ["Connor Skennerton"]
__license__ = "GPL3"
__version__ = "0.0.1"
__maintainer__ = "Connor Skennerton"
__email__ = "c.skennerton@gmail.com"
__status__ = "Development"

###############################################################################

import argparse
import sys
import tempfile
import os
import shutil
import subprocess
import textwrap
from collections import defaultdict
import amplishot.parse.fastx
from amplishot import taxon_segregator
from amplishot.app import bowtie, phrap, pandaseq

###############################################################################
###############################################################################
###############################################################################
###############################################################################

###############################################################################
###############################################################################
###############################################################################
###############################################################################


def map_to_reference(inreads, bowtiedb, threads):
    """Take overlapped reads from pandaseq and map them to reference sequences
       returns path to output file containing sam formated alignments
       inreads: path to file containing overlapped reads from pandaseq
       bowtiedb: path to fileprefix of the bowtie database
    """
    b = amplishot.app.bowtie.Bowtie(U=inreads, index=bowtiedb, threads=threads)
    print b
    stdout, stderr = b(stderr=False)
    #sam = tempfile.NamedTemporaryFile(suffix='.sam', delete=False)
    #with open(os.devnull, 'w') as fnull:
    #    subprocess.call(['bowtie2', '-U', inreads, '-x', bowtiedb, '-p',
    #        str(threads)], stdout = sam, stderr = fnull)
    #sam.close()
    return stdout, stderr


def taxon_partition(samfile, taxonomyFile, rootdir='root'):
    """Call the taxon segregator and return a list of tuples of taxonomies that
       have adequate coverage for assembly
    """
    taxseg = amplishot.taxon_segregator.TaxonSegregator(taxonomyFile)
    taxseg.parse_sam(samfile)
    return taxseg.segregate(root=rootdir)


def cd_hit_reduce(workingdir, infile='in.fa', outfile='cdhitout.fa'):
    proc = subprocess.Popen(["cd-hit-est","-c","0.98", "-M", "1000","-o",
                os.path.join(workingdir,outfile),"-i",os.path.join(workingdir,infile)],
            stdout=subprocess.PIPE, stderr=subprocess.PIPE)

    stdout, stderr = proc.communicate()
    return_value = proc.returncode
    return stdout, stderr, return_value


def process_cd_hit_results(directory, cdhitout='cdhitout.fa', cdhitin='in.fa'):
    clustered_qual = open(os.path.join(directory,cdhitout+'.qual'), 'w')
    clustered_reads = set()
    cd_hit_fasta = open(os.path.join(directory,cdhitout))
    fxparser = amplishot.parse.fastx.FastxReader(cd_hit_fasta)
    for name, seq, qual in fxparser.parse():
        clustered_reads.add(name)

    cd_hit_fasta.close()
    inqual = open(os.path.join(directory,cdhitin+'.qual'))
    qualparser = amplishot.parse.fastx.QualityReader(inqual)
    for name, qual in qualparser.parse():
        if name in clustered_reads:
            clustered_qual.write('>%s\n%s\n' % (name, qual))
    inqual.close()


def phrap_assemble(workingdir, infile='cdhitout.fa',
        suppressStdout=True, suppressStderr=True, minscore=300):
    p = phrap.Phrap(infile=infile, minscore=minscore)
    print p
    stdout, stderr = p(stdout=suppressStdout, stderr=suppressStderr,
           cwd=workingdir)

    return stdout, stderr, p.get_result_paths()
    #out = None
    #err = None
    #if suppressStdout:
    #    out = open(os.devnull, "w")
    #if suppressStderr:
    #    err = open(os.devnull, "w")
    #return subprocess.call(['phrap', infile, '-minscore','300'],
    #        stderr=err, stdout=out, cwd=workingdir)

def demultiplex(args):
    pass
def assemble(args):
    pass


def overlap(args):
    panda = pandaseq.Pandaseq(overlap=args.overlap, minlength=args.minlength)
    stdout, stderr = panda()
    return stdout, stderr


def map_reads(args):
    pass

def filter_sam(args): 
    taxseg = amplishot.taxon_segregator.TaxonSegregator(args.taxonomy)
    for samfile in args.sam:
        taxseg.parse_sam(samfile, args.percentid)
    good_taxons, bad_taxons = taxseg.segregate(root=args.outdir, minCount=args.taxon_width,
            minCoverage=args.taxon_depth)
    

def cluster(args): pass
def pipeline(args):
    """Run all the steps of amplishot
    """
    root_dir = args.outdir
    # generate fragment consensus sequences - pandaseq
    #panda_out = panda(args)

    # map overlapped fragments with bowtie
    print 'mapping with bowtie...'
    sams = []
    for samfile in args.file:
        sams.append(map_to_reference(samfile, args.bowtiedb, args.threads))

    try:
        index = 0
        # create the file for all the full-length seqs
        full_length_seqs =\
        open(os.path.join(root_dir, 'combined_samples.full_length.fa'), 'w')
        for samfile in sams:
            print 'processing %s...' % args.file[index]
            sample_file_name =\
            os.path.splitext(os.path.basename(args.file[index]))[0]
            taxon_root = os.path.join(args.outdir,
                sample_file_name, 'root')

            # partition dataset based on mapped reads
            print '\tpartitioning...'
            good_taxons, bad_taxons = taxon_partition(samfile, args.taxonomy,
                    rootdir=taxon_root)
            print '\tThere are %i taxons suitable for assembly'\
                    ' and %i taxons with incomplete coverage'\
                    % (len(good_taxons), len(bad_taxons))
            if len(good_taxons) < 1:
                print "\tNo taxons for assembly"
            else:
                good_taxons = [os.path.join(taxon_root, *x) for x in good_taxons]
                
                # dataset reduction - cd-hit - each partition - threaded
                print '\tclustering...'
                for t in good_taxons:
                    cd_hit_reduce(t, infile='reads.fa')
                
                print '\tassembling...'
                for t in good_taxons:
                    process_cd_hit_results(t, cdhitin='reads.fa')

                ## generate overlaps - phrap - each partition - threaded
                for t in good_taxons:
                    stdout, stderr, results = phrap_assemble(t)
                    # collect all full-length sequences
                    with open(results['contigs']) as fp:
                        fxparser = amplishot.parse.fastx.FastxReader(fp)
                        full_length_counter = 1
                        for name, seq, qual in\
                        fxparser.parse(callback=amplishot.parse.fastx.greater_than,
                                length=1000):
                            full_length_seqs.write('>%s_%i\n%s\n' %
                                    (sample_file_name, full_length_counter,
                                    seq))
                            full_length_counter += 1
            i += 1

        full_length_seqs.close()
        # pick OTUs and representative set
        out_otu_map = amplishot.otu_table.pick_otus(full_length_seqs.name,
                outputFilePath=os.path.join(root_dir, 'full_length_otus.txt'))
        rep_set = amplishot.otu_table.pick_rep_set(full_length_seqs.name, out_otu_map,
                outputFilePath=os.path.join(root_dir, 'full_length_rep_set.fa'))
        rep_set_seqs = dict()
        with open(rep_set) as fp:
            fxparser = amplishot.parse.fastx(fp)
            for name, seq, qual in fxparser.parse():
                rep_set_seqs[name] = seq

        #assign taxonomy
        sam_alignments, bowtie_error = map_to_reference(rep_set,
                args.bowtiedb, args.threads)
        taxon_mapper = amplishot.taxon_segregator(args.taxonomy)
        taxon_mapper.parse_sam(sam_alignments)
        sam_alignments.close()
        bowtie_error.close()

        observation_metadata = defaultdict(dict)  #[{} for i in range(len(abundance.seqs))]
        for taxon, otu_representatives in taxon_mapper.taxon_mapping.items():
            tax_string =\
            amplishot.assign_taxonomy.get_greengenes_tax_string(taxon)
            for otu in otu_representatives:
                observation_metadata[otu]['taxonomy'] = tax_string
        
        # create Bowtie database from representative set
        abundance = amplishot.otu_table.OTUTableGenerator(rep_set_seqs,
                root_dir)
        #generate abundance based on coverage and create OTU table
        for read_file in args.file:
            abundance.generate_abundance(read_file)
        
        abundance.add_metadata(observation_metadata=observation_metadata)
        
        print abundance


    finally:
        for sam in sams:
            if os.path.exists(sam):
                os.remove(sam)


###############################################################################
###############################################################################
###############################################################################
###############################################################################

if __name__ == '__main__':
   
#--------------------
    #place options that will be used for all subparsers here
    common_options_parser = argparse.ArgumentParser(add_help=False)
    common_options_parser.add_argument('-c', '--config-file', dest='config',
            default=os.path.join(os.path.expanduser('~'),'.amplishot_config.yml'),
            help='specify a custom location to the amplishot configuration\
            file. By default all values stored in the config file will\
            be used unless they are overwritten by command-line options\
            or if the --suppress-config-file option is set')
    common_options_parser.add_argument('--supress-config-file',
    action='store_true', default=False, help='Do not use any values stored in\
            the config file')

#--------------------
    # create a threads parser specifically to hold that option, then we can
    # pass it to the parsers that need it
    threads_parser = argparse.ArgumentParser(add_help=False)
    threads_parser.add_argument('-t', '--threads', type=int, dest='threads',
            default=1, help='The number of threads to use')

#--------------------
    # top level parser for running the pipeline
    # amplishot [OPTIONS] <file>...
    parser = argparse.ArgumentParser(
            prog='amplishot',
            formatter_class=argparse.RawDescriptionHelpFormatter,
            description=textwrap.dedent("""\
            %(prog)s is a pipeline for reconstructing full length 16S sequences 
            from specially prepared 'amplishot' libraries.
            
            There are two ways to run amplishot:
            1. First is to perform each step individually using the subcommands 
            listed at the bottom of this help message.
            
            2. The other way is as a pipline, which will take the
            raw reads from the sequencer and give you back full-length 16S
            sequences.  If you want to use the pipline simply leave out the
            subcommand name and supply the options and files nessessary.  If
            you want to perform each step manually refer to the subcommand help
            by invoking '%(prog)s pipeline -h'
            """)
            )
    subparser = parser.add_subparsers(title='Commands',
            metavar=' ',
            dest='subparser_name')

#--------------------
    #pipeline parser
    # amplishot pipeline [OPTIONS] file [file...]
    pipeline_parser = subparser.add_parser('pipeline',
            help='Run %(prog)s from start to finish',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            parents=[common_options_parser, threads_parser]
            )
    pipeline_parser.add_argument('-T','--taxonomy', required=True, dest='taxonomy', help="File\
            containing the taxonomy mapping of the reference sequences in the\
                    bowtie database")
    pipeline_parser.add_argument('-d', '--bowtie-database', required=True, dest='bowtiedb', help="the\
            path to the bowtie database, without the suffixes; just like you\
            would write when using bowtie")
    pipeline_parser.add_argument('-o', '--output-directory',dest='outdir', default='.',
            help='Root directoy under which all files and folders from the\
            pipeline will be generated from')
    pipeline_parser.add_argument('file',nargs='+',
            help='Input files containing read pairs')
    pipeline_parser.set_defaults(func=pipeline)



#--------------------
    # demultiplexing parser
    # amplishot demultiplex [OPTIONS] {<file1> <file2>}...
    demultiplex_parser = subparser.add_parser('demultiplex', 
            help='Split datasets based on user defined barcode sequences', 
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            parents=[common_options_parser]
            )
    demultiplex_parser.add_argument('-o', '--output-directory', default='.', dest='outdir',
            help='Specify an output directory for demutiplexed files')
    demultiplex_parser.add_argument('-b', '--barcode-mapping', dest='barcodes',
            required=True, help="""A file containing barcodes to search for.
            The file must be tab separated with each barcode on a single line.
            The first column should be an identifier for the barcode, the
            second column should be the sequence of the barcode.  If the read 
            pair is dual barcoded then a third column can be
            specified as the barcode for the reverse read.  Note that both
            barcodes should be in the correct orientation for their read (i.e.
            the second barcode should be reversed complemented)
            """)
    demultiplex_parser.add_argument('file', nargs='+', 
            help='Input file(s) that need to be demultiplexed they may be\
                    in fasta, fastq and be gziped')
    demultiplex_parser.set_defaults(func=demultiplex)

#--------------------
    # overlap parser
    # amplishot overlap [OPTIONS] {<file1> <file2>}...
    overlap_parser = subparser.add_parser('overlap', 
            help='overlap read pairs to generate a consensus sequence',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            parents=[common_options_parser, threads_parser]
            )
    overlap_parser.add_argument('-o', '--output-directory',
            default='./overlapped', dest='outdir', 
            help='Output directory for the consensus fragments')
    overlap_parser.add_argument('-l', '--min-length', default=350,
            dest='minlength', help='The minimal acceptable length for\
            assembled pairs')
    overlap_parser.add_argument('-O', '--overlap-length', default=30,
            dest='overlap', 
            help='the minimum length that pairs must overlap by')
    overlap_parser.add_argument('file', nargs='+',
            help='Input must be pairs of files that contain the forward\
            and reverse reads for assembly into consensus fragments')
    overlap_parser.set_defaults(func=overlap)

#--------------------
    # mapping parser
    # amplishot map [OPTIONS] <file>...
    map_parser = subparser.add_parser('map',
            help='map assembled pairs to a reference database',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            parents=[common_options_parser, threads_parser]
            )
    map_parser.add_argument('-x', '--index', dest='db', required=True,
            help='Path to the index file conatining the 16S database.\
            Must be in bowtie2 format')
    map_parser.add_argument('-o', '--out-sam', dest='sam', required=True,
            help='output file in sam format for alignments')
    map_parser.add_argument('file', nargs='+',
            help='input files containing assembled fragments')
    map_parser.set_defaults(func=map_reads)

#--------------------
    # filtering parser
    # amplishot filter [OPTIONS] <file.sam>...
    filter_parser = subparser.add_parser('filter',
            help='segregate reads based on the similarity and taxonomy\
            of reference sequences',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            parents=[common_options_parser]
            )
    filter_parser.add_argument('-o', '--output-directory', dest='outdir',
            default='.', help='output directory prefix')
    filter_parser.add_argument('-T', '--taxonomy-file', required=True,
            dest='taxonomy', help='A mapping file in either Greengenes or\
            SILVA format that maps the reference IDs to their taxonomies')
    filter_parser.add_argument('-p', '--percent-id', type=float, default=0.98,
            dest='percentid',
            help='The global percentage Identity that a read must have to the\
            reference sequence. A good rule of thumb is to set this value to\
            be 1% lower than the clustering percentage used on the reference\
            database.')
    filter_parser.add_argument('-C', '--taxon-depth', type=int, default=2,
            dest='taxon_depth',
            help='After taxonomic filtering, this value dictates the minimum\
            coverage for EVERY covered base in the references of that taxon\
            must contain to be considered suitable for assembly')
    filter_parser.add_argument('-m', '--minimum-bases', type=int, default=1000,
            dest='taxon_width',
            help='After taxonomic filtering, this value dictates the minimum\
            number of bases that must have query bases mapping to them for\
            the entire taxon')
    filter_parser.add_argument('sam', nargs='+',
            help='sam formatted files containing assembled fragments for\
            taxonomic filtering')
    filter_parser.set_defaults(func=filter_sam)

#--------------------
    #clustering parser
    # amplishot cluster [OPTIONS] <file>...
    cluster_parser = subparser.add_parser('cluster',
            help='Remove redundant coverage in taxons.  Greating decreases\
            assembly time',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            parents=[common_options_parser]
            )
    cluster_parser.add_argument('-p', '--percent-identity', type=float,
            default=0.98, dest='percentid',
            help='The percent identity that two reads must have to be clustered')
    cluster_parser.add_argument('file', nargs='+',
            help='fasta files containing reads to be clustered')
    cluster_parser.set_defaults(func=cluster)

#--------------------
    # assembly parser
    # amplishot assemble [OPTIONS] <file.fa>...
    assembly_parser = subparser.add_parser('assemble',
            help='Assemble individual taxons into full-length 16S sequences',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            parents=[common_options_parser]
            )
    assembly_parser.add_argument('-m', '--min-score', dest='minscore',
            type=int, default=300,
            help='The minimum alignment score used in phrap')
    assembly_parser.add_argument('file', nargs='+',
            help='Path to files containing taxonomies for assembly')
    assembly_parser.set_defaults(func=assemble)

    args = parser.parse_args()
    args.func(args)


###############################################################################
###############################################################################
###############################################################################
###############################################################################
