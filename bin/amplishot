#!/usr/bin/python
###############################################################################
#
# amplishot.py - Pipeline for generating full-length 16S sequences and
#                performing community abundance measurements on the output
#
###############################################################################
#                                                                             #
#    This program is free software: you can redistribute it and/or modify     #
#    it under the terms of the GNU General Public License as published by     #
#    the Free Software Foundation, either version 3 of the License, or        #
#    (at your option) any later version.                                      #
#                                                                             #
#    This program is distributed in the hope that it will be useful,          #
#    but WITHOUT ANY WARRANTY; without even the implied warranty of           #
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the            #
#    GNU General Public License for more details.                             #
#                                                                             #
#    You should have received a copy of the GNU General Public License        #
#    along with this program. If not, see <http://www.gnu.org/licenses/>.     #
#                                                                             #
###############################################################################

__author__ = "Connor Skennerton"
__copyright__ = "Copyright 2012-2013"
__credits__ = ["Connor Skennerton"]
__license__ = "GPL3"
__version__ = "0.3.2"
__maintainer__ = "Connor Skennerton"
__email__ = "c.skennerton@gmail.com"
__status__ = "Development"

###############################################################################
import logging
import argparse
import sys
import tempfile
import os
import time
import shutil
import subprocess
import textwrap
import gzip
import multiprocessing
from collections import defaultdict
import amplishot.parse.fastx
from amplishot import taxon_segregator, otu_table, assign_taxonomy, assemble,\
        conserved_sequences
from amplishot.app import bowtie,pandaseq
from amplishot.config import AmplishotConfig
from cogent.app.cd_hit import CD_HIT_EST
from cogent.app.util import FilePath
###############################################################################

def map_to_reference(params=None, stderr=None,
        stdout=None):
    """Take overlapped reads from pandaseq and map them to reference sequences
       returns path to output file containing sam formated alignments
       inreads: path to file containing overlapped reads from pandaseq
       bowtiedb: path to fileprefix of the bowtie database
    """
    b = amplishot.app.bowtie.Bowtie2(params=params, SuppressStderr=True,
            SuppressStdout=True)
    logging.debug(str(b))
    return b(stderr=stderr, stdout=stdout)

def taxon_partition(samfile, taxonomyFile, rootdir='root'):
    """Call the taxon segregator and return a list of tuples of taxonomies that
       have adequate coverage for assembly
    """
    taxseg = amplishot.taxon_segregator.TaxonSegregator(taxonomyFile)
    taxseg.parse_sam(samfile)
    return taxseg.segregate(root=rootdir)


def demultiplex(args):
    pass


def overlap(forward, reverse, pairtig_dir, minPairtigLen=350, minOverlap=30):
    pairtig_file = os.path.join(pairtig_dir,
        os.path.splitext(os.path.basename(forward))[0] + '.pairtigs.fq')
    with open(pairtig_file, 'wb') as fp:
        panda = pandaseq.Pandaseq()
        panda.Parameters['-f'].on(forward)
        panda.Parameters['-r'].on(reverse)
        panda.Parameters['-l'].on(minPairtigLen)
        panda.Parameters['-o'].on(minOverlap)
        panda.Parameters['-F'].on()
        panda(stdout=fp, stderr=None)
    return pairtig_file


def filter_sam(args): 
    taxseg = amplishot.taxon_segregator.TaxonSegregator(args.taxonomy)
    for samfile in args.sam:
        taxseg.parse_sam(samfile, args.percentid)
    good_taxons, bad_taxons = taxseg.segregate(root=args.outdir, minCount=args.taxon_width,
            minCoverage=args.taxon_depth)
    
def make_aliases(config):
    indata = []
    config.data['aliases'] = []
    for readfile in config.data['pairtig_read_files']:
        sample_file_name =\
        os.path.splitext(os.path.basename(readfile))[0]
        indata.append((readfile, sample_file_name))
        config.data['aliases'].append(sample_file_name)

    return indata


def orientate(fullLengthSeqs):
    cs = conserved_sequences.ConservedSequences()
    for name in fullLengthSeqs.keys():
        fullLengthSeqs[name] = cs.orientate(fullLengthSeqs[name])

def pipeline(config):
    """Run all the steps of amplishot
    """
    root_dir = config.data['output_directory'] = os.path.abspath(config.data['output_directory'])

    # generate fragment consensus sequences - pandaseq
    pairtig_dir = os.path.join(root_dir, 'pairtigs')
    if not os.path.exists(pairtig_dir):
        os.mkdir(pairtig_dir)

    logging.info('generating pairtigs...')
    # lets not fork unnessessary processes
    panda_processes = len(config.data['input_raw_reads']) if\
    len(config.data['input_raw_reads']) < config.data['threads'] else\
    config.data['threads']

    panda_pool = multiprocessing.Pool(processes=panda_processes)
    panda_pool_results = [panda_pool.apply_async(overlap, (f, r, pairtig_dir), 
        dict(minPairtigLen=config.data['minimum_pairtig_length'], 
            minOverlap=config.data['pair_overlap_length']) ) for f, r in
            config.data['input_raw_reads']]

    panda_pool.close()
    panda_pool.join()

    reads_counts = []
    for result in panda_pool_results:
        pairtig_file = result.get()
        with open(pairtig_file) as fp:
            parser = amplishot.parse.fastx.FastxReader(fp)
            read_count = 0
            for name, seq, qual in parser.parse():
                read_count += 1
            reads_counts.append(read_count)
            r = result.get()
        config.data['pairtig_read_files'].append(os.path.abspath(r))

    # take the smallest number of pairtigs and round down to the nearest 100
    if len(reads_counts) > 1:
        x = min(reads_counts)
        normalized_read_count = x if x % 100 == 0 else x - 100 - x % 100
    else:
        normalized_read_count = reads_counts[0]
    # map overlapped fragments with bowtie
    # create the file for all the full-length seqs
    full_length_seqs =\
    open(os.path.join(root_dir, 
        config.data.get('reconstruced_seq_file', 'full_length_sequences.fa')), 'w')

    # taxonomy assigner
    taxseg = amplishot.taxon_segregator.TaxonSegregator(config.data['taxonomy_file'],
            cutoffs=config.data['mapping_similarity_cutoffs'])
    
    # fix up the aliases
    indata = []
    if config.data['aliases']:
        if len(config.data['aliases']) != len(config.data['pairtig_read_files']):
            logging.warn('There are a different number of aliases and input'\
                'files. Aliases will not be used')
            indata = make_aliases(config)
        else:
            indata = zip(config.data['pairtig_read_files'], config.data['aliases'])
    else:
        indata = make_aliases(config)
    # assembly wrapper
    wrapper = assemble.AssemblyWrapper(config.data['assembly_method'], config,
            preAssembleReduction=config.data['preassemble_clustering'])
    for readfile, samplename in indata:
        logging.info('processing %s...', readfile)
        sample_mapping_file = os.path.join(root_dir, samplename + '.initial_mapping.sam')
        bowtie_results = map_to_reference( params={'-U': readfile,
            '-x': config.data['mapper_database'],
            '-S': sample_mapping_file,
            '-p': config.data['threads'],
            #'-u': 10000,
            })

        # partition dataset based on mapped reads
        logging.info('partitioning...')
        taxon_root = os.path.join(root_dir, samplename, 'root')
        taxseg.parse_sam(open(sample_mapping_file))
        bowtie_results.cleanUp()

        good_taxons, bad_taxons = taxseg.segregate(root=taxon_root, 
                minCount=config.data['taxon_coverage'][1], 
                minCoverage=config.data['taxon_coverage'][0],
                sam=True)
        taxseg.clear()

        if len(good_taxons) < 1:
            logging.info("No taxons for assembly")
        else:
            logging.info('There are %i taxons suitable for assembly'\
                    ' and %i taxons with incomplete coverage',
                    len(good_taxons), len(bad_taxons))

            if good_taxons.has_key(()) and not config.data['assemble_unknowns']:
                del good_taxons[()]

            taxons_for_assembly = {}
            for taxon, cutoffs in good_taxons.items():
                taxons_for_assembly[os.path.join(taxon_root, *taxon)] = cutoffs
            
            wrapper(taxons_for_assembly, samplename)

    # fix up the orientation of the full-length sequences to they are all in
    # the forward direction then write them to file
    orientate(wrapper.fullLengthSeqs)
    wrapper.write(full_length_seqs)

    full_length_seqs.close()
    # pick OTUs and representative set
    out_otu_map = amplishot.otu_table.pick_otus(full_length_seqs.name,
            outputFileName=os.path.join(root_dir, 'full_length_otus.txt'),
            similarity=config.data['otu_clustering_similarity'])
    rep_set = amplishot.otu_table.pick_rep_set(full_length_seqs.name, out_otu_map,
            outputFilePath=os.path.join(root_dir,
                config.data.get('repset_output_file',
                    'full_length_sequences.repset.fa')))
    rep_set_seqs = dict()
    with open(rep_set) as fp:
        fxparser = amplishot.parse.fastx.FastxReader(fp)
        for name, seq, qual in fxparser.parse():
            rep_set_seqs[name] = seq

    logging.info('There are %i OTUs, assigning taxonomy...', len(rep_set_seqs))
    
    #assign taxonomy
    taxonomy_map = assign_taxonomy.assign_taxonomy(config.data['assign_taxonomy_method'],
            rep_set, config)

    observation_metadata = defaultdict(dict)
    score_field_name = 'confidence'
    if config.data['assign_taxonomy_method'] == 'blast':
        score_field_name = 'evalue'
    elif config.data['assign_taxonomy_method'] == 'bowtie':
        score_field_name = 'similarity'
    for otu, metadata in taxonomy_map.items():
        observation_metadata[otu]['taxonomy'] = metadata[0].split('; ')
        observation_metadata[otu][score_field_name] = metadata[1]

    logging.info('generating OTU table in biom format...')
    
    # create Bowtie database from representative set
    abundance = amplishot.otu_table.OTUTableGenerator(rep_set_seqs,
            root_dir)
    #generate abundance based on coverage and create OTU table
    _params = {'-p': config.data['threads'], '-u': normalized_read_count}
    for read_file, sample_name in indata:
        abundance.generate_abundance(read_file, alias=sample_name,
                params=_params)
    
    abundance.generate_biom_table()
    abundance.add_metadata(observation_metadata=observation_metadata)
    with open(os.path.join(root_dir, 'abundances.biom'), 'w') as fp:
        fp.write(str(abundance))
    
    if config.data['normalize_otu_table']:
        abundance.normalize()
        with open(os.path.join(root_dir, 'abundances.normalized.biom'), 'w') as fp:
            fp.write(str(abundance))

    logging.info('Done.')

def write_config(config):
    # check for absolute paths; change them if they are not
    config.data['input_raw_reads'] = [[os.path.abspath(e) for e in r] for r in\
            config.data['input_raw_reads']]
    current_time = time.strftime('%Y%m%d%H%M%S')
    outfp = os.path.join(config.data['output_directory'], 'Amplishot_%s_config.yml' %
            current_time)
    with open(outfp, 'w') as fp:
        fp.write(str(config))


def check_config_and_set_output(args, config):
    if args.config is not None:
        config.populate_from_config_file(open(args.config))
        initial_config_str = str(config)

    config.populate_from_commandline(args)
    try:
        root_dir = config.data['output_directory']
    except KeyError: 
        root_dir = config.data['output_directory'] = os.getcwd()

    if not os.path.exists(root_dir):
        os.makedirs(root_dir)

    config_str = str(config)
    if initial_config_str != config_str:
        return True
    return False

###############################################################################
###############################################################################
###############################################################################
###############################################################################

if __name__ == '__main__':
#-------------------
    # initialise the config parser
    config = AmplishotConfig()

#--------------------
    #place options that will be used for all subparsers here
    common_options_parser = argparse.ArgumentParser(add_help=False)
    common_options_parser.add_argument('-c', '--config-file', dest='config',
            #default=os.path.join(os.path.expanduser('~'),'.amplishot_config.yml'),
            help='specify a custom location to the amplishot configuration\
            file. By default all values stored in the config file will\
            be used unless they are overwritten by command-line options')
    common_options_parser.add_argument('--quiet', dest='logger_quiet',
            action='store_const', const='CRITICAL', help='change the log verbosity so that only'\
                    'the most critical messages are reported')
    common_options_parser.add_argument('--log-level', choices=['DEBUG', 'INFO',
        'WARNING', 'ERROR', 'CRITICAL'], dest='log_level',
        help='Set the log level')
    common_options_parser.add_argument('--log-file', dest='log_file',
            help='Change the destination of logging.')

#--------------------
    # create a threads parser specifically to hold that option, then we can
    # pass it to the parsers that need it
    threads_parser = argparse.ArgumentParser(add_help=False)
    threads_parser.add_argument('-t', '--threads', type=int, dest='threads',
             help='The number of threads to use')

#--------------------
    # top level parser for running the pipeline
    # amplishot [OPTIONS] <file>...
    parser = argparse.ArgumentParser(
            prog='amplishot',
            formatter_class=argparse.RawDescriptionHelpFormatter,
            description=textwrap.dedent("""\
            %(prog)s is a pipeline for reconstructing full length 16S sequences 
            from specially prepared 'amplishot' libraries.
            
            There are two ways to run amplishot:
            1. First is to perform each step individually using the subcommands 
            listed at the bottom of this help message.
            
            2. The other way is as a pipline, which will take the
            raw reads from the sequencer and give you back full-length 16S
            sequences.  If you want to use the pipline simply leave out the
            subcommand name and supply the options and files nessessary.  If
            you want to perform each step manually refer to the subcommand help
            by invoking '%(prog)s pipeline -h'
            """)
            )
    subparser = parser.add_subparsers(title='Commands',
            metavar=' ',
            dest='subparser_name')

#--------------------
    #pipeline parser
    # amplishot pipeline [OPTIONS] file [file...]
    pipeline_parser = subparser.add_parser('pipeline',
            help='Run %(prog)s from start to finish',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            parents=[common_options_parser, threads_parser]
            )
    pipeline_parser.add_argument('-T','--taxonomy', #required=True,
    dest='taxonomy_file', help="File\
            containing the taxonomy mapping of the reference sequences in the\
                    bowtie database")
    pipeline_parser.add_argument('-d', '--bowtie-database', #required=True,
    dest='mapper_database', help="the\
            path to the bowtie database, without the suffixes; just like you\
            would write when using bowtie")
    pipeline_parser.add_argument('-o',
            '--output-directory', dest='output_directory',
            help='Root directoy under which all files and folders from the\
            pipeline will be generated from')
    #pipeline_parser.add_argument('-m', '--minscore', dest='phrap_minscore',
    #        type=int, default=config.data.get('phrap_minscore', 300),
    #        help='The minimum alignment score used in\
    #        the phrap assembly')
    pipeline_parser.add_argument('-s', '--similarity', type=float,
            dest='otu_clustering_similarity',
            help='The similarity'\
                    ' required between sequences AFTER reconstruction'\
                    ' for OTU clustering')
    pipeline_parser.add_argument('-i', '--input', nargs=2, action='append',
            help='paths to first and pecond pairs of reads for \'pairtig\'\
            generation.  Can be specified multiple times for many samples',
            dest='input_raw_reads')
    pipeline_parser.add_argument('-l', '--min-length',
            dest='minimum_pairtig_length',
            help='The minimal acceptable length for\
            assembled pairs')
    pipeline_parser.add_argument('-O', '--overlap-length',
            dest='pair_overlap_length',
            help='the minimum length that pairs must overlap by')
    pipeline_parser.set_defaults(func=pipeline)

    args = parser.parse_args()

    write_new_config = check_config_and_set_output(args, config)
#-------------------
    # set up the logger
    if args.logger_quiet is not None:
        config.data['log_level'] = args.logger_quiet

    numeric_level = getattr(logging, config.data['log_level'], None)
    if not isinstance(numeric_level, int):
        raise ValueError('Invalid log level: %s' % config.data['log_level'])

    logging.basicConfig(level=numeric_level,
            format='%(levelname)s\t%(asctime)s\t%(message)s')
    
    if config.data['log_file'] is not None:
        logging.basicConfig(filename=config.data['log_file'])

    args.func(config)
    if write_new_config:
        write_config(config)


###############################################################################
###############################################################################
###############################################################################
###############################################################################
